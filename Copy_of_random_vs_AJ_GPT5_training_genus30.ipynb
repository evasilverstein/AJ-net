{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "11/22/2025 In this notebook, we benchmark againt a different control (aside from the 2g x 2g baseline): a randomly generated activation going from 2 \\to 2g, comparing apples to apples against our best genus 30 AJ models on MNIST. This sort of test was suggested by Mike Douglas.  A random fourier features setup does well but not as well (and an MLP alternative for the 2\\to2g does not do well).   AJ ~92% vs RFF ~ 88 % according to initial runs.  To do:  inject this into Noam's now much faster pipeline which has AJ working easily up to genus 150 on MNIST (as predicted competitively to 2g x 2g but with order 2g parameters, much fewer as we go up in genus).\n",
        "\n"
      ],
      "metadata": {
        "id": "OWzNjEzKqd3p"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1gYMuYlSxM2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOBwLptQbR-M",
        "outputId": "6f923b92-98a5-428a-8e15-8b1f9a831247"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Loaded: genus=30, grid=(96x96)\n"
          ]
        }
      ],
      "source": [
        "# @title Load from Drive\n",
        "!pip install -q torch torchvision numpy\n",
        "\n",
        "import os, math, json\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "from google.colab import drive\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ===========================\n",
        "# 1) Drive + Paths (match Notebook 1)\n",
        "# ===========================\n",
        "DRIVE_FOLDER = \"AJ_Tables_g30\"  # same as Notebook 1\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "SAVE_DIR = f\"/content/drive/MyDrive/{DRIVE_FOLDER}\"\n",
        "INTEGRALS_PATH = os.path.join(SAVE_DIR, \"aj_integrals_genus30.pt\")\n",
        "OMEGAS_PATH    = os.path.join(SAVE_DIR, \"aj_omegas_genus30.pt\")\n",
        "assert os.path.exists(INTEGRALS_PATH), \"Integrals file not found in Drive\"\n",
        "assert os.path.exists(OMEGAS_PATH),    \"Omegas file not found in Drive\"\n",
        "\n",
        "\n",
        "\n",
        "# ===========================\n",
        "# 2) Load tables\n",
        "# ===========================\n",
        "ints = torch.load(INTEGRALS_PATH, map_location='cpu', weights_only=False)\n",
        "omeg = torch.load(OMEGAS_PATH,    map_location='cpu', weights_only=False)\n",
        "\n",
        "# ints = torch.load(INTEGRALS_PATH, map_location='cpu')  ### old version\n",
        "# omeg = torch.load(OMEGAS_PATH,    map_location='cpu')\n",
        "\n",
        "genus       = int(ints[\"genus\"])\n",
        "grid_r_np   = np.array(ints[\"grid_r\"])\n",
        "grid_i_np   = np.array(ints[\"grid_i\"])\n",
        "branch_pts  = np.array(ints[\"branch_pts\"])\n",
        "I_plus      = ints[\"I_plus\"]            # (g, H, W) complex\n",
        "Om_plus     = omeg[\"omega_plus\"]        # (g, H, W) complex\n",
        "\n",
        "H, W = I_plus.shape[-2:]\n",
        "grid_r = torch.tensor(grid_r_np, dtype=torch.float32)\n",
        "grid_i = torch.tensor(grid_i_np, dtype=torch.float32)\n",
        "branch_pts_t = torch.tensor(branch_pts)   # complex128 → complex64 in torch if needed\n",
        "\n",
        "print(f\"Loaded: genus={genus}, grid=({H}x{W})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Base class AJMNIST_Anchored (needed for PeriodicHead)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class AJMNIST_Anchored(nn.Module):\n",
        "    \"\"\"\n",
        "    Anchored AJ model:\n",
        "      - conv trunk (1→64)\n",
        "      - learned per-point embeddings (g×embed_dim)\n",
        "      - shared point_head producing (x,y, sheet_logit) for each point\n",
        "      - per-point bias initialized to ω-aware anchors\n",
        "      - AJGridActivationNorm does lookup, standardization, sheet sign, and sum\n",
        "      - classifier: linear 2g→10\n",
        "    \"\"\"\n",
        "    def __init__(self, genus, I_plus, Om_plus, grid_r, grid_i, branch_pts,\n",
        "                 anchors_xy, mu, sigma, embed_dim=8):\n",
        "        super().__init__()\n",
        "        self.genus = genus\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.AdaptiveAvgPool2d((1,1))\n",
        "        )\n",
        "        self.embed = nn.Parameter(torch.empty(genus, embed_dim))\n",
        "        nn.init.uniform_(self.embed, -2.0, 2.0)\n",
        "\n",
        "        # shared point_head (no bias) + per-point bias\n",
        "        self.point_head = nn.Linear(64 + embed_dim, 3, bias=False)\n",
        "        nn.init.xavier_uniform_(self.point_head.weight)\n",
        "        self.point_bias = nn.Parameter(torch.zeros(genus, 3))\n",
        "\n",
        "        # AJ activation with normalization\n",
        "        self.aj = AJGridActivationNorm(I_plus, Om_plus, grid_r, grid_i, branch_pts, mu, sigma)\n",
        "        self.classifier = nn.Linear(2*genus, 10)\n",
        "\n",
        "        # ω-aware initialization for biases\n",
        "        rmin, rmax = float(grid_r.min()), float(grid_r.max())\n",
        "        imin, imax = float(grid_i.min()), float(grid_i.max())\n",
        "        def logit(p): return float(torch.log(p/(1-p)))\n",
        "        with torch.no_grad():\n",
        "            for i in range(genus):\n",
        "                x0, y0 = float(anchors_xy[i,0]), float(anchors_xy[i,1])\n",
        "                px = (x0 - rmin) / (rmax - rmin)\n",
        "                py = (y0 - imin) / (imax - imin)\n",
        "                self.point_bias[i, 0] = logit(torch.tensor(px))\n",
        "                self.point_bias[i, 1] = logit(torch.tensor(py))\n",
        "                target_sign = 0.8 if (i % 2 == 0) else -0.8\n",
        "                self.point_bias[i, 2] = float(torch.atanh(torch.tensor(target_sign)))\n",
        "\n",
        "    def forward(self, x, return_aux=False):\n",
        "        B = x.size(0)\n",
        "        h = self.conv(x).view(B, -1)\n",
        "        h_exp = h.unsqueeze(1).expand(-1, self.genus, -1)\n",
        "        emb   = self.embed.unsqueeze(0).expand(B, -1, -1)\n",
        "        inp   = torch.cat([h_exp, emb], dim=2)\n",
        "\n",
        "        out   = self.point_head(inp) + self.point_bias.unsqueeze(0)  # (B,g,3)\n",
        "        raw_xy, sheet_logits = out[..., :2], out[..., 2]\n",
        "        coords, aux = self.aj(raw_xy, sheet_logits, return_aux=True)\n",
        "        logits = self.classifier(coords)\n",
        "        if return_aux:\n",
        "            return logits, aux\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "ZdfC2DNCzLXe"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title AJ model with ω-aware init + AJ normalization + learnable gain\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def _pack_complex_table(table_gHW: torch.Tensor) -> torch.Tensor:\n",
        "    re, im = table_gHW.real, table_gHW.imag\n",
        "    return torch.cat([re, im], dim=0).unsqueeze(0).contiguous()  # (1, 2g, H, W)\n",
        "\n",
        "class AJGridActivationNorm(nn.Module):\n",
        "    \"\"\"\n",
        "    AJ lookup with:\n",
        "      - bilinear sampling\n",
        "      - continuous sheet sign via tanh\n",
        "      - channel-wise standardization: (I - mu)/sigma\n",
        "      - learnable global gain gamma\n",
        "      - boundary/branch penalties (for diagnostics/regularization)\n",
        "    \"\"\"\n",
        "    def __init__(self, I_plus, Om_plus, grid_r, grid_i, branch_pts, mu, sigma):\n",
        "        super().__init__()\n",
        "        self.g = I_plus.shape[0]\n",
        "        self.register_buffer(\"I_plus\",  _pack_complex_table(I_plus))\n",
        "        self.register_buffer(\"Om_plus\", _pack_complex_table(Om_plus))\n",
        "\n",
        "        # stats\n",
        "        self.register_buffer(\"mu\",    mu.view(1, 1, -1))     # (1,1,2g)\n",
        "        self.register_buffer(\"sigma\", sigma.view(1, 1, -1))  # (1,1,2g)\n",
        "        self.gamma = nn.Parameter(torch.tensor(1.0))         # learnable gain\n",
        "\n",
        "        # bounds\n",
        "        self.register_buffer(\"r_min\", torch.tensor(float(grid_r.min())))\n",
        "        self.register_buffer(\"r_max\", torch.tensor(float(grid_r.max())))\n",
        "        self.register_buffer(\"i_min\", torch.tensor(float(grid_i.min())))\n",
        "        self.register_buffer(\"i_max\", torch.tensor(float(grid_i.max())))\n",
        "\n",
        "        # branch points\n",
        "        self.register_buffer(\"bp_real\", branch_pts.real.float())\n",
        "        self.register_buffer(\"bp_imag\", branch_pts.imag.float())\n",
        "\n",
        "    def _map_raw_to_bounds(self, raw_xy):\n",
        "        # sigmoid mapping into box\n",
        "        xr = self.r_min + (self.r_max - self.r_min) * torch.sigmoid(raw_xy[..., 0])\n",
        "        yi = self.i_min + (self.i_max - self.i_min) * torch.sigmoid(raw_xy[..., 1])\n",
        "        return xr, yi\n",
        "\n",
        "    def _norm_to_grid(self, xr, yi):\n",
        "        gx = 2.0 * (xr - self.r_min) / (self.r_max - self.r_min) - 1.0\n",
        "        gy = 2.0 * (yi - self.i_min) / (self.i_max - self.i_min) - 1.0\n",
        "        return gx, gy\n",
        "\n",
        "    def forward(self, raw_xy: torch.Tensor, sheet_logits: torch.Tensor, return_aux=True):\n",
        "        B, g, _ = raw_xy.shape\n",
        "        assert g == self.g\n",
        "\n",
        "        xr, yi = self._map_raw_to_bounds(raw_xy)\n",
        "        gx, gy = self._norm_to_grid(xr, yi)\n",
        "        grid = torch.stack([gx, gy], dim=-1).view(B*g, 1, 1, 2)\n",
        "\n",
        "        # Sample integrals and standardize\n",
        "        I = F.grid_sample(self.I_plus.expand(B*g, -1, -1, -1),\n",
        "                          grid, mode=\"bilinear\", align_corners=True).view(B, g, -1)\n",
        "        I_std = (I - self.mu) / self.sigma                        # (B, g, 2g)\n",
        "\n",
        "        # Continuous sheet sign (initialized away from 0 in the constructor below)\n",
        "        #sign = torch.tanh(sheet_logits).unsqueeze(-1)             # (B, g, 1)\n",
        "        # inside AJGridActivationNorm.forward(...)\n",
        "\n",
        "        sign = torch.tanh(sheet_logits).unsqueeze(-1)  # (B,g,1)\n",
        "        contrib = sign * I_std\n",
        "        coords  = self.gamma * contrib.sum(dim=1)      # (B,2g)\n",
        "\n",
        "\n",
        "        # contrib = sign * I_std\n",
        "        # coords = self.gamma * contrib.sum(dim=1)                  # (B, 2g)\n",
        "\n",
        "        aux = None\n",
        "        if return_aux:\n",
        "            margin = 0.95\n",
        "            bpen = ((gx.abs() - margin).clamp_min(0)**2 +\n",
        "                    (gy.abs() - margin).clamp_min(0)**2).mean()\n",
        "\n",
        "            dx = xr.unsqueeze(-1) - self.bp_real\n",
        "            dy = yi.unsqueeze(-1) - self.bp_imag\n",
        "            d2 = dx*dx + dy*dy\n",
        "            tau = 0.07\n",
        "            rpen = torch.exp(-d2 / (2*tau*tau)).mean()\n",
        "\n",
        "            # aux = {\"bound_penalty\": bpen, \"branch_penalty\": rpen,\n",
        "            #        \"gx\": gx, \"gy\": gy, \"x\": xr, \"y\": yi}\n",
        "            # inside AJGridActivationNorm.forward(...), in the return_aux block\n",
        "            aux = {\n",
        "                \"x\": xr, \"y\": yi, \"gx\": gx, \"gy\": gy,\n",
        "                \"bound_penalty\": bpen, \"branch_penalty\": rpen,\n",
        "                \"sheet_sign\": sign.squeeze(-1)   # keep this\n",
        "                # \"omega_channels\": Om  <-- delete this line\n",
        "            }\n",
        "\n",
        "            # aux = {\n",
        "            #     \"x\": xr, \"y\": yi, \"gx\": gx, \"gy\": gy,\n",
        "            #     \"bound_penalty\": bpen, \"branch_penalty\": rpen,\n",
        "            #     \"omega_channels\": Om,\n",
        "            #     \"sheet_sign\": sign.squeeze(-1)  # <-- add this line\n",
        "            # }\n",
        "        return coords, aux\n",
        "\n",
        "def logit(p):  # inverse sigmoid\n",
        "    p = np.clip(p, 1e-6, 1-1e-6)\n",
        "    return float(np.log(p/(1-p)))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "o_xhCmbWBww6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AJ periodic head WITHOUT tau: axis-aligned Fourier features (global K)\n",
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "\n",
        "class TorusFeatures(nn.Module):\n",
        "    def __init__(self, dim: int, K: int = 2, freqs=None, learnable: bool = False):\n",
        "        super().__init__()\n",
        "        if freqs is None:\n",
        "            freqs = torch.tensor([0.5, 1.0], dtype=torch.float32)[:K]\n",
        "        else:\n",
        "            freqs = torch.as_tensor(freqs, dtype=torch.float32)[:K]\n",
        "        if learnable:\n",
        "            self.freqs = nn.Parameter(freqs)\n",
        "        else:\n",
        "            self.register_buffer(\"freqs\", freqs)\n",
        "        self.dim, self.K = dim, len(freqs)\n",
        "    def forward(self, u):\n",
        "        B, D = u.shape\n",
        "        f = (self.freqs if isinstance(self.freqs, torch.Tensor) else torch.as_tensor(self.freqs)).view(1,1,-1).to(u.device)\n",
        "        ang = u.unsqueeze(-1) * f\n",
        "        return torch.cat([torch.cos(ang), torch.sin(ang)], dim=-1).view(B, D*2*self.K)\n",
        "\n",
        "class AJMNIST_AxisPeriodic(nn.Module):\n",
        "    \"\"\"\n",
        "    Drop-in head: AJ (anchored+norm) → axis-aligned periodic features → classifier\n",
        "    Expects you already have AJMNIST_Anchored and AJGridActivationNorm defined.\n",
        "    \"\"\"\n",
        "    def __init__(self, genus, I_plus, Om_plus, grid_r, grid_i, branch_pts,\n",
        "                 anchors_xy, mu, sigma, embed_dim=8, K=2, learnable_freqs=False):\n",
        "        super().__init__()\n",
        "        self.base = AJMNIST_Anchored(genus, I_plus, Om_plus, grid_r, grid_i, branch_pts,\n",
        "                                     anchors_xy, mu, sigma, embed_dim=embed_dim)\n",
        "        D = 2*genus\n",
        "        self.torus = TorusFeatures(D, K=K, learnable=learnable_freqs)\n",
        "        self.classifier = nn.Linear(D*2*K, 10)\n",
        "    @property\n",
        "    def genus(self): return self.base.genus\n",
        "    def forward(self, x, return_aux=False):\n",
        "        B = x.size(0)\n",
        "        h = self.base.conv(x).view(B, -1)\n",
        "        h_exp = h.unsqueeze(1).expand(-1, self.genus, -1)\n",
        "        emb   = self.base.embed.unsqueeze(0).expand(B, -1, -1)\n",
        "        out   = self.base.point_head(torch.cat([h_exp, emb], dim=2)) + self.base.point_bias.unsqueeze(0)\n",
        "        raw_xy, sheet_logits = out[..., :2], out[..., 2]\n",
        "        coords, aux = self.base.aj(raw_xy, sheet_logits, return_aux=True)  # (B, 2g)\n",
        "        feats = self.torus(coords)\n",
        "        logits = self.classifier(feats)\n",
        "        if return_aux: return logits, aux\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "gQ2Dt5Z3Bw2g"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Compute ω-aware anchors and AJ normalization\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# 1) Build a scalar \"ω-strength\" map on the grid to avoid dead-gradient regions\n",
        "#    Use sum_k |ω_k| (or |ω_k|^2); both work similarly.\n",
        "with torch.no_grad():\n",
        "    # Om_plus: (g, H, W) complex\n",
        "    Wmap = Om_plus.abs().sum(dim=0)  # (H, W)\n",
        "    Wmap_np = Wmap.cpu().numpy()\n",
        "\n",
        "# 2) Mask out boundaries and branch points (stay away from peaks/singularities too)\n",
        "H, W = Wmap_np.shape\n",
        "gy = np.linspace(-1, 1, H)[:, None]\n",
        "gx = np.linspace(-1, 1, W)[None, :]\n",
        "edge_mask = (np.abs(gx) < 0.92) & (np.abs(gy) < 0.92)\n",
        "\n",
        "# Distance from branch points in grid coordinates\n",
        "grid_x = grid_r_np[None, :].repeat(H, axis=0)  # (H,W)\n",
        "grid_y = grid_i_np[:, None].repeat(W, axis=1)  # (H,W)\n",
        "bp_real = np.real(branch_pts).reshape(-1, 1, 1)\n",
        "bp_imag = np.imag(branch_pts).reshape(-1, 1, 1)\n",
        "d2 = (grid_x - bp_real)**2 + (grid_y - bp_imag)**2  # (P, H, W)\n",
        "bp_mask = (d2.min(axis=0) > 0.25)  # keep points with dist > ~0.5\n",
        "\n",
        "mask = edge_mask & bp_mask\n",
        "\n",
        "# 3) Pick g anchor points among top-quantile ω regions, well separated\n",
        "score = np.where(mask, Wmap_np, -np.inf)\n",
        "th = np.quantile(score[score > -np.inf], 0.85)  # top 15% by ω-strength\n",
        "cand = np.argwhere(score >= th)                 # list of (iy, ix)\n",
        "\n",
        "# Farthest-point sampling to pick 'genus' diverse anchors\n",
        "def farthest_k(points_hw, k):\n",
        "    pts = points_hw.copy()\n",
        "    # Start from the global maximum\n",
        "    start = pts[np.argmax(score[tuple(pts.T)])]\n",
        "    chosen = [start]\n",
        "    if k == 1: return np.array(chosen)\n",
        "    # Precompute physical coordinates for distances\n",
        "    coords = np.stack([grid_x[tuple(pts.T)], grid_y[tuple(pts.T)]], axis=1)\n",
        "    c0 = np.array([grid_x[start[0], start[1]], grid_y[start[0], start[1]]])[None, :]\n",
        "    mind = np.sum((coords - c0)**2, axis=1)\n",
        "    for _ in range(1, k):\n",
        "        j = np.argmax(mind)\n",
        "        chosen.append(pts[j])\n",
        "        cj = coords[j][None, :]\n",
        "        mind = np.minimum(mind, np.sum((coords - cj)**2, axis=1))\n",
        "    return np.array(chosen)\n",
        "\n",
        "anchors_hw = farthest_k(cand, genus)  # shape (g, 2) with (iy, ix)\n",
        "\n",
        "# 4) Convert anchors to (x0, y0) coordinates\n",
        "x0 = grid_r_np[anchors_hw[:, 1]]\n",
        "y0 = grid_i_np[anchors_hw[:, 0]]\n",
        "anchors_xy = np.stack([x0, y0], axis=1)  # (g, 2)\n",
        "\n",
        "# 5) Compute per-channel mean/std of AJ coordinates for standardization\n",
        "#    Pack integrals into 2g channels [Re..., Im...], then compute stats over H×W.\n",
        "I_re = I_plus.real    # (g, H, W)\n",
        "I_im = I_plus.imag    # (g, H, W)\n",
        "I_ch = torch.cat([I_re, I_im], dim=0)  # (2g, H, W)\n",
        "mu = I_ch.mean(dim=(1, 2))             # (2g,)\n",
        "sigma = I_ch.std(dim=(1, 2)).clamp_min(1e-6)\n",
        "\n",
        "# Save for later use\n",
        "anchors_xy_t = torch.tensor(anchors_xy, dtype=torch.float32)\n",
        "mu_t = mu.float()\n",
        "sigma_t = sigma.float()\n",
        "\n",
        "print(\"Chosen anchors (x0,y0):\")\n",
        "for i, (x, y) in enumerate(anchors_xy):\n",
        "    print(f\"  point {i:2d}: x0={x:+.3f}, y0={y:+.3f}\")\n",
        "print(\"\\nAJ normalization ready: per-channel mean/std computed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDVFUrT-BxIR",
        "outputId": "4c3adc4c-280b-4bb7-a8bf-678cd086743c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chosen anchors (x0,y0):\n",
            "  point  0: x0=+2.084, y0=-4.232\n",
            "  point  1: x0=-0.442, y0=+5.495\n",
            "  point  2: x0=-5.242, y0=-0.947\n",
            "  point  3: x0=+4.358, y0=+1.453\n",
            "  point  4: x0=-2.463, y0=-4.611\n",
            "  point  5: x0=-3.853, y0=+2.968\n",
            "  point  6: x0=+4.611, y0=-1.958\n",
            "  point  7: x0=+2.463, y0=+4.105\n",
            "  point  8: x0=-3.600, y0=-2.589\n",
            "  point  9: x0=-0.189, y0=-4.989\n",
            "  point 10: x0=-4.989, y0=+1.200\n",
            "  point 11: x0=-2.084, y0=+3.979\n",
            "  point 12: x0=+3.979, y0=-3.853\n",
            "  point 13: x0=+1.200, y0=+5.116\n",
            "  point 14: x0=+1.200, y0=-5.242\n",
            "  point 15: x0=-4.863, y0=-2.211\n",
            "  point 16: x0=-3.474, y0=-3.853\n",
            "  point 17: x0=-4.737, y0=+0.063\n",
            "  point 18: x0=+3.095, y0=-4.737\n",
            "  point 19: x0=-1.200, y0=+4.611\n",
            "  point 20: x0=-4.232, y0=+1.958\n",
            "  point 21: x0=-3.095, y0=+3.726\n",
            "  point 22: x0=+4.105, y0=-2.842\n",
            "  point 23: x0=-1.453, y0=-4.484\n",
            "  point 24: x0=+2.968, y0=-3.726\n",
            "  point 25: x0=+1.453, y0=+4.232\n",
            "  point 26: x0=+0.316, y0=+4.863\n",
            "  point 27: x0=+0.695, y0=-4.484\n",
            "  point 28: x0=+2.211, y0=-5.116\n",
            "  point 29: x0=-4.358, y0=-0.821\n",
            "\n",
            "AJ normalization ready: per-channel mean/std computed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# 5) Data\n",
        "# ===========================\n",
        "tfm = T.Compose([T.ToTensor(), T.Normalize((0.1307,), (0.3081,))])\n",
        "train_ds = torchvision.datasets.MNIST(root=\"/content/data\", train=True, download=True, transform=tfm)\n",
        "test_ds  = torchvision.datasets.MNIST(root=\"/content/data\", train=False, download=True, transform=tfm)\n",
        "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2, pin_memory=True)\n",
        "test_loader  = torch.utils.data.DataLoader(test_ds,  batch_size=256, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "# ===========================\n",
        "# 6) Train / Eval\n",
        "# ===========================\n",
        "def train_epoch(model, loader, opt, clip=1.0, lam_branch=1e-3, lam_bound=1e-3):\n",
        "    model.train()\n",
        "    ce = nn.CrossEntropyLoss()\n",
        "    tot, correct, n = 0.0, 0, 0\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        logits, aux = model(x, return_aux=True)\n",
        "\n",
        "        loss = ce(logits, y)\n",
        "        loss = loss + lam_branch * aux[\"branch_penalty\"] + lam_bound * aux[\"bound_penalty\"]\n",
        "        loss.backward()\n",
        "        if clip is not None:\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        opt.step()\n",
        "\n",
        "        tot += loss.item() * x.size(0)\n",
        "        correct += (logits.argmax(1) == y).sum().item()\n",
        "        n += x.size(0)\n",
        "    return tot/n, 100.0*correct/n\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_epoch(model, loader):\n",
        "    model.eval()\n",
        "    ce = nn.CrossEntropyLoss()\n",
        "    tot, correct, n = 0.0, 0, 0\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        tot += ce(logits, y).item() * x.size(0)\n",
        "        correct += (logits.argmax(1) == y).sum().item()\n",
        "        n += x.size(0)\n",
        "    return tot/n, 100.0*correct/n"
      ],
      "metadata": {
        "id": "5AWlGtNwWoXK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "223dea75-1874-49a0-c624-728a9dff63d4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 12.6MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 338kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 3.19MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 8.44MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Train τ-free axis-periodic AJ model with smaller batch & AMP (OOM-safe)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from collections import defaultdict\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# -------------------- sanity: required globals --------------------\n",
        "required = [\n",
        "    \"genus\", \"I_plus\", \"Om_plus\", \"grid_r\", \"grid_i\", \"branch_pts_t\",\n",
        "    \"anchors_xy_t\", \"mu_t\", \"sigma_t\",\n",
        "    \"train_loader\", \"test_loader\",\n",
        "    \"AJMNIST_AxisPeriodic\"\n",
        "]\n",
        "for name in required:\n",
        "    if name not in globals():\n",
        "        raise RuntimeError(f\"Missing required object `{name}` before training cell.\")\n",
        "\n",
        "print(f\"Current genus = {genus}\")\n",
        "\n",
        "# -------------------- build smaller DataLoaders to save memory --------------------\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "orig_train_loader = train_loader\n",
        "orig_test_loader  = test_loader\n",
        "\n",
        "# Use a conservative batch size (e.g. 64) for AJ model\n",
        "BASE_BS = 64\n",
        "orig_bs = getattr(orig_train_loader, \"batch_size\", None)\n",
        "small_bs = BASE_BS if (orig_bs is None) else min(BASE_BS, orig_bs)\n",
        "\n",
        "train_loader_axis = DataLoader(\n",
        "    orig_train_loader.dataset,\n",
        "    batch_size=small_bs,\n",
        "    shuffle=True,\n",
        "    num_workers=getattr(orig_train_loader, \"num_workers\", 0),\n",
        "    pin_memory=getattr(orig_train_loader, \"pin_memory\", False),\n",
        "    drop_last=True\n",
        ")\n",
        "test_loader_axis = DataLoader(\n",
        "    orig_test_loader.dataset,\n",
        "    batch_size=min(256, small_bs*2),\n",
        "    shuffle=False,\n",
        "    num_workers=getattr(orig_test_loader, \"num_workers\", 0),\n",
        "    pin_memory=getattr(orig_test_loader, \"pin_memory\", False),\n",
        "    drop_last=False\n",
        ")\n",
        "\n",
        "print(f\"Original train batch size: {orig_bs}\")\n",
        "print(f\"AJ-axis train batch size : {small_bs}\")\n",
        "\n",
        "# -------------------- helpers --------------------\n",
        "def count_params(m: nn.Module) -> int:\n",
        "    return sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
        "\n",
        "def heads_only_params(m: nn.Module) -> int:\n",
        "    # conv trunk is in m.base.conv\n",
        "    total = count_params(m)\n",
        "    conv_params = sum(p.numel() for p in m.base.conv.parameters() if p.requires_grad)\n",
        "    return total - conv_params\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_epoch(model: nn.Module, loader):\n",
        "    model.eval()\n",
        "    ce = nn.CrossEntropyLoss()\n",
        "    tot, correct, n = 0.0, 0, 0\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        tot += ce(logits, y).item() * x.size(0)\n",
        "        correct += (logits.argmax(1) == y).sum().item()\n",
        "        n += x.size(0)\n",
        "    return tot/n, 100.0*correct/n\n",
        "\n",
        "# -------------------- AMP-aware training epoch --------------------\n",
        "USE_AMP = (device.type == \"cuda\")\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n",
        "\n",
        "def train_epoch_amp(model: nn.Module, loader, opt,\n",
        "                    clip: float = 1.0,\n",
        "                    lam_branch: float = 1e-3,\n",
        "                    lam_bound:  float = 1e-3):\n",
        "    model.train()\n",
        "    ce = nn.CrossEntropyLoss()\n",
        "    tot, correct, n = 0.0, 0, 0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        with torch.cuda.amp.autocast(enabled=USE_AMP):\n",
        "            logits, aux = model(x, return_aux=True)\n",
        "            loss = ce(logits, y)\n",
        "            if aux is not None:\n",
        "                loss = loss + lam_branch*aux.get(\"branch_penalty\", 0.0) \\\n",
        "                             + lam_bound *aux.get(\"bound_penalty\",  0.0)\n",
        "        scaler.scale(loss).backward()\n",
        "        if clip is not None:\n",
        "            scaler.unscale_(opt)\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        scaler.step(opt)\n",
        "        scaler.update()\n",
        "\n",
        "        tot     += loss.item() * x.size(0)\n",
        "        correct += (logits.argmax(1) == y).sum().item()\n",
        "        n       += x.size(0)\n",
        "    return tot/n, 100.0*correct/n\n",
        "\n",
        "# optional AJ diagnostics hook if you defined one earlier\n",
        "def maybe_aj_diags(model, loader, n_batches=2):\n",
        "    if \"aj_diags\" in globals():\n",
        "        aj_diags(model, loader, device, n_batches=n_batches)\n",
        "\n",
        "# -------------------- instantiate axis-periodic AJ model --------------------\n",
        "K = 2                 # number of Fourier harmonics per coordinate\n",
        "LEARN_FREQS = False   # fixed frequencies for τ-free head\n",
        "EMBED_DIM  = 4        # small embedding to keep params modest\n",
        "\n",
        "aj_axis = AJMNIST_AxisPeriodic(\n",
        "    genus,\n",
        "    I_plus, Om_plus,\n",
        "    grid_r, grid_i,\n",
        "    branch_pts_t,\n",
        "    anchors_xy_t.to(device),\n",
        "    mu_t.to(device), sigma_t.to(device),\n",
        "    embed_dim=EMBED_DIM,\n",
        "    K=K,\n",
        "    learnable_freqs=LEARN_FREQS\n",
        ").to(device)\n",
        "\n",
        "total_params = count_params(aj_axis)\n",
        "head_params  = heads_only_params(aj_axis)\n",
        "print(f\"\\nAJ axis-periodic (g={genus}, K={K}) params: {total_params:,}\")\n",
        "print(f\"  heads-only (total minus conv trunk): {head_params:,}\")\n",
        "\n",
        "# -------------------- optimizer (two-tier LR) --------------------\n",
        "fast_params = list(aj_axis.base.point_head.parameters()) + \\\n",
        "              [aj_axis.base.point_bias] + \\\n",
        "              list(aj_axis.torus.parameters()) + \\\n",
        "              list(aj_axis.classifier.parameters())\n",
        "\n",
        "base_params = list(aj_axis.base.conv.parameters()) + \\\n",
        "              list(aj_axis.base.aj.parameters())\n",
        "\n",
        "opt = torch.optim.AdamW(\n",
        "    [\n",
        "        {\"params\": base_params, \"lr\": 3e-4},\n",
        "        {\"params\": fast_params, \"lr\": 1e-3},\n",
        "    ],\n",
        "    weight_decay=1e-4\n",
        ")\n",
        "\n",
        "# -------------------- training loop --------------------\n",
        "EPOCHS     = 8\n",
        "CLIP_NORM  = 1.0\n",
        "LAM_BRANCH = 1e-3\n",
        "LAM_BOUND  = 1e-3\n",
        "\n",
        "print(\"\\nStarting training with AMP =\", USE_AMP)\n",
        "for ep in range(1, EPOCHS+1):\n",
        "    tr_loss, tr_acc = train_epoch_amp(\n",
        "        aj_axis, train_loader_axis, opt,\n",
        "        clip=CLIP_NORM,\n",
        "        lam_branch=LAM_BRANCH,\n",
        "        lam_bound=LAM_BOUND\n",
        "    )\n",
        "    te_loss, te_acc = eval_epoch(aj_axis, test_loader_axis)\n",
        "    print(f\"[AJ axis K={K}] Epoch {ep:02d} | \"\n",
        "          f\"train {tr_loss:.4f} / {tr_acc:.2f}% | \"\n",
        "          f\"test {te_loss:.4f} / {te_acc:.2f}%\")\n",
        "    maybe_aj_diags(aj_axis, train_loader_axis, n_batches=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhY1ctYtcWuj",
        "outputId": "93cbcaf3-227f-457f-ed02-1a3e859427b3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Current genus = 30\n",
            "Original train batch size: 128\n",
            "AJ-axis train batch size : 64\n",
            "\n",
            "AJ axis-periodic (g=30, K=2) params: 22,251\n",
            "  heads-only (total minus conv trunk): 3,435\n",
            "\n",
            "Starting training with AMP = True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-290682449.py:80: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n",
            "/tmp/ipython-input-290682449.py:93: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=USE_AMP):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[AJ axis K=2] Epoch 01 | train 1.5287 / 43.11% | test 0.9165 / 67.98%\n",
            "[AJ axis K=2] Epoch 02 | train 0.8225 / 71.01% | test 0.5715 / 80.85%\n",
            "[AJ axis K=2] Epoch 03 | train 0.6117 / 79.12% | test 0.4531 / 85.51%\n",
            "[AJ axis K=2] Epoch 04 | train 0.4956 / 83.48% | test 0.3597 / 88.29%\n",
            "[AJ axis K=2] Epoch 05 | train 0.4202 / 86.33% | test 0.4384 / 85.59%\n",
            "[AJ axis K=2] Epoch 06 | train 0.3756 / 87.84% | test 0.3611 / 88.01%\n",
            "[AJ axis K=2] Epoch 07 | train 0.3244 / 89.66% | test 0.2530 / 92.23%\n",
            "[AJ axis K=2] Epoch 08 | train 0.2728 / 91.40% | test 0.2718 / 91.28%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title RFF–AxisPeriodic (simple): conv → 2 → RFF(2→4gK) → 10  | AMP + small-batch\n",
        "\n",
        "import math, torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# ---------------- device & sanity ----------------\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device:\", device)\n",
        "assert 'genus' in globals(), \"Please define `genus` first (e.g., genus = 30).\"\n",
        "assert 'train_loader' in globals() and 'test_loader' in globals(), \"Please create train/test loaders first.\"\n",
        "print(f\"Genus = {genus} → 2g = {2*genus}\")\n",
        "\n",
        "# -------------- small-batch loaders --------------\n",
        "BASE_BS = 64\n",
        "orig_train_loader = train_loader\n",
        "orig_test_loader  = test_loader\n",
        "orig_bs = getattr(orig_train_loader, \"batch_size\", None)\n",
        "small_bs = BASE_BS if (orig_bs is None) else min(BASE_BS, orig_bs)\n",
        "\n",
        "train_loader_rff_axis = DataLoader(\n",
        "    orig_train_loader.dataset,\n",
        "    batch_size=small_bs, shuffle=True,\n",
        "    num_workers=getattr(orig_train_loader, \"num_workers\", 0),\n",
        "    pin_memory=getattr(orig_train_loader, \"pin_memory\", False),\n",
        "    drop_last=True\n",
        ")\n",
        "test_loader_rff_axis = DataLoader(\n",
        "    orig_test_loader.dataset,\n",
        "    batch_size=min(256, small_bs*2), shuffle=False,\n",
        "    num_workers=getattr(orig_test_loader, \"num_workers\", 0),\n",
        "    pin_memory=getattr(orig_test_loader, \"pin_memory\", False),\n",
        "    drop_last=False\n",
        ")\n",
        "print(f\"RFF-axis train batch size: {train_loader_rff_axis.batch_size}\")\n",
        "\n",
        "# ---------------- helpers (scoped) ----------------\n",
        "def make_conv_trunk_rffaxis():\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(1, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
        "        nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
        "        nn.AdaptiveAvgPool2d((1, 1))\n",
        "    )\n",
        "\n",
        "def init_two_band_rffaxis_(tensor, low=1.0, high=3.0):\n",
        "    with torch.no_grad():\n",
        "        sign = (torch.randint(0, 2, tensor.shape, device=tensor.device)*2 - 1).float()\n",
        "        mag  = torch.empty_like(tensor, dtype=torch.float32).uniform_(low, high)\n",
        "        tensor.copy_(sign * mag)\n",
        "\n",
        "def count_params(m):\n",
        "    return sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
        "\n",
        "def conv_param_count(m):\n",
        "    return sum(p.numel() for n,p in m.named_parameters()\n",
        "               if p.requires_grad and n.startswith(\"conv\"))\n",
        "\n",
        "def heads_only_params(m):\n",
        "    return count_params(m) - conv_param_count(m)\n",
        "\n",
        "# ---------------- model (simple, no norm/dropout) ----------------\n",
        "class RFFAxisPeriodicSimple(nn.Module):\n",
        "    \"\"\"\n",
        "    conv(1→64) → Linear(64→2) →\n",
        "      RFF lift: z ↦ [cos(Wz+b), sin(Wz+b)]\n",
        "        with M = 2 g K so D_feat = 2M = 4 g K (matches AJ axis-periodic head)\n",
        "      → Linear(D_feat→10)\n",
        "\n",
        "    - W ∈ R^{M×2}, b ∈ R^M are frozen (random features)\n",
        "    - No BN/LN/Dropout in the head (like earlier RFF baselines that trained well)\n",
        "    \"\"\"\n",
        "    def __init__(self, genus: int, K: int = 2, omega_std: float = 1.0, rff_seed: int = 1234):\n",
        "        super().__init__()\n",
        "        self.genus, self.K = genus, K\n",
        "        g = genus\n",
        "\n",
        "        # conv + 2D bottleneck\n",
        "        self.conv = make_conv_trunk_rffaxis()\n",
        "        self.fc_down = nn.Linear(64, 2)\n",
        "        init_two_band_rffaxis_(self.fc_down.weight); nn.init.zeros_(self.fc_down.bias)\n",
        "\n",
        "        # RFF frequencies & phases (frozen)\n",
        "        M = 2 * g * K                # M such that D_feat = 2M = 4 g K\n",
        "        gen = torch.Generator().manual_seed(rff_seed)\n",
        "        W = torch.randn(M, 2, generator=gen) * omega_std\n",
        "        b = 2*math.pi * torch.rand(M, generator=gen)\n",
        "        self.register_buffer(\"W\", W.float())     # (M,2)\n",
        "        self.register_buffer(\"b\", b.float())     # (M,)\n",
        "\n",
        "        # Classifier on 4 g K features\n",
        "        self.classifier = nn.Linear(2*M, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.size(0)\n",
        "        h = self.conv(x).view(B, -1)                 # (B,64)\n",
        "        z = self.fc_down(h)                          # (B,2)\n",
        "\n",
        "        W = self.W.to(z.device, dtype=z.dtype)\n",
        "        b = self.b.to(z.device, dtype=z.dtype)\n",
        "        theta = z @ W.T + b                          # (B, M)\n",
        "        feats = torch.cat([torch.cos(theta), torch.sin(theta)], dim=1)  # (B, 2M)\n",
        "\n",
        "        logits = self.classifier(feats)              # (B,10)\n",
        "        return logits\n",
        "\n",
        "# --------------- AMP training utils ---------------\n",
        "USE_AMP = (device.type == 'cuda')\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n",
        "ce = nn.CrossEntropyLoss()\n",
        "\n",
        "def train_epoch_amp(model, loader, opt, clip=1.0):\n",
        "    model.train()\n",
        "    tot, correct, n = 0.0, 0, 0\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        with torch.cuda.amp.autocast(enabled=USE_AMP):\n",
        "            logits = model(x)\n",
        "            loss   = ce(logits, y)\n",
        "        scaler.scale(loss).backward()\n",
        "        if clip is not None:\n",
        "            scaler.unscale_(opt)\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        scaler.step(opt); scaler.update()\n",
        "        tot     += loss.item() * x.size(0)\n",
        "        correct += (logits.argmax(1) == y).sum().item()\n",
        "        n       += x.size(0)\n",
        "    return tot/n, 100.0*correct/n\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_epoch(model, loader):\n",
        "    model.eval()\n",
        "    tot, correct, n = 0.0, 0, 0\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        loss   = ce(logits, y)\n",
        "        tot     += loss.item() * x.size(0)\n",
        "        correct += (logits.argmax(1) == y).sum().item()\n",
        "        n       += x.size(0)\n",
        "    return tot/n, 100.0*correct/n\n",
        "\n",
        "# --------------- instantiate & train ---------------\n",
        "K = 2               # match AJ axis-periodic harmonics\n",
        "OMEGA_STD = 1.0     # RFF bandwidth; 0.6–1.2 are typical\n",
        "\n",
        "rff_axis_simple = RFFAxisPeriodicSimple(genus, K=K, omega_std=OMEGA_STD).to(device)\n",
        "\n",
        "total = count_params(rff_axis_simple)\n",
        "conv  = conv_param_count(rff_axis_simple)\n",
        "heads = heads_only_params(rff_axis_simple)\n",
        "print(f\"\\nRFF–AxisPeriodic SIMPLE (g={genus}, K={K})\")\n",
        "print(f\"  total params : {total:,}\")\n",
        "print(f\"  conv trunk   : {conv:,}\")\n",
        "print(f\"  heads-only   : {heads:,}  (total minus conv trunk)\")\n",
        "\n",
        "# Local (model-only) LRs — TUNE HERE if needed\n",
        "BASE_LR = 3e-4   # conv trunk\n",
        "HEAD_LR = 1e-3   # fc_down + classifier  ← increase to 1.5e-3 or 2e-3 if it's flat\n",
        "\n",
        "base_params = list(rff_axis_simple.conv.parameters())\n",
        "fast_params = list(rff_axis_simple.fc_down.parameters()) + list(rff_axis_simple.classifier.parameters())\n",
        "\n",
        "opt_rff_axis_simple = torch.optim.AdamW(\n",
        "    [{\"params\": base_params, \"lr\": BASE_LR},\n",
        "     {\"params\": fast_params, \"lr\": HEAD_LR}],\n",
        "    weight_decay=1e-4\n",
        ")\n",
        "print(f\"Using BASE_LR={BASE_LR:.1e}, HEAD_LR={HEAD_LR:.1e}\")\n",
        "\n",
        "EPOCHS, CLIP = 8, 1.0\n",
        "print(\"\\nStarting RFF–AxisPeriodic SIMPLE training (AMP =\", USE_AMP, \")\")\n",
        "for ep in range(1, EPOCHS+1):\n",
        "    tr_loss, tr_acc = train_epoch_amp(rff_axis_simple, train_loader_rff_axis, opt_rff_axis_simple, clip=CLIP)\n",
        "    te_loss, te_acc = eval_epoch(rff_axis_simple, test_loader_rff_axis)\n",
        "    print(f\"[RFF–AxisPeriodic SIMPLE K={K}] Ep {ep:02d} | \"\n",
        "          f\"train {tr_loss:.4f}/{tr_acc:.2f}% | test {te_loss:.4f}/{te_acc:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwsQ0gyIojYx",
        "outputId": "ecf1b3de-708c-42e7-bca7-4d792524d371"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Genus = 30 → 2g = 60\n",
            "RFF-axis train batch size: 64\n",
            "\n",
            "RFF–AxisPeriodic SIMPLE (g=30, K=2)\n",
            "  total params : 21,356\n",
            "  conv trunk   : 18,816\n",
            "  heads-only   : 2,540  (total minus conv trunk)\n",
            "Using BASE_LR=3.0e-04, HEAD_LR=1.0e-03\n",
            "\n",
            "Starting RFF–AxisPeriodic SIMPLE training (AMP = True )\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3211729141.py:109: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n",
            "/tmp/ipython-input-3211729141.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=USE_AMP):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RFF–AxisPeriodic SIMPLE K=2] Ep 01 | train 1.2317/50.80% | test 0.8181/69.36%\n",
            "[RFF–AxisPeriodic SIMPLE K=2] Ep 02 | train 0.8063/69.63% | test 0.6601/75.70%\n",
            "[RFF–AxisPeriodic SIMPLE K=2] Ep 03 | train 0.6733/75.07% | test 0.6995/75.03%\n",
            "[RFF–AxisPeriodic SIMPLE K=2] Ep 04 | train 0.6128/77.97% | test 0.5532/80.57%\n",
            "[RFF–AxisPeriodic SIMPLE K=2] Ep 05 | train 0.5592/80.51% | test 0.5604/80.69%\n",
            "[RFF–AxisPeriodic SIMPLE K=2] Ep 06 | train 0.5173/82.38% | test 0.4555/85.17%\n",
            "[RFF–AxisPeriodic SIMPLE K=2] Ep 07 | train 0.4899/83.38% | test 0.4275/85.87%\n",
            "[RFF–AxisPeriodic SIMPLE K=2] Ep 08 | train 0.4726/84.17% | test 0.3813/87.61%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Compact τ-free axis-periodic AJ model (definition + training)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math, os, time\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# --------- sanity checks for precomputed objects ---------\n",
        "needed = [\n",
        "    \"genus\", \"I_plus\", \"Om_plus\", \"grid_r\", \"grid_i\", \"branch_pts_t\",\n",
        "    \"anchors_xy_t\", \"mu_t\", \"sigma_t\", \"AJMNIST_Anchored\", \"TorusFeatures\",\n",
        "    \"train_loader\", \"test_loader\"\n",
        "]\n",
        "for name in needed:\n",
        "    if name not in globals():\n",
        "        raise RuntimeError(f\"Missing `{name}`; make sure your genus-{globals().get('genus','?')} setup cells ran.\")\n",
        "\n",
        "print(f\"Current genus = {genus}\")\n",
        "\n",
        "# If you already have smaller-batch loaders from the previous cell, reuse them:\n",
        "train_loader_compact = train_loader_axis if 'train_loader_axis' in globals() else train_loader\n",
        "test_loader_compact  = test_loader_axis  if 'test_loader_axis'  in globals() else test_loader\n",
        "print(\"train_loader_compact batch_size:\", train_loader_compact.batch_size)\n",
        "\n",
        "# --------- helper: fixed orthogonal projector ---------\n",
        "def make_fixed_orthogonal(D: int, r: int, seed: int = 1234):\n",
        "    \"\"\"\n",
        "    Returns a D x r matrix with orthonormal columns (Q), as float32.\n",
        "    Used as a non-trainable bottleneck from feature dim D to r.\n",
        "    \"\"\"\n",
        "    assert r <= D, f\"r={r} must be <= D={D}\"\n",
        "    gen = torch.Generator().manual_seed(seed)\n",
        "    A = torch.randn(D, r, generator=gen)\n",
        "    Q, _ = torch.linalg.qr(A, mode='reduced')  # D x r\n",
        "    return Q[:, :r].float()\n",
        "\n",
        "# --------- compact axis-periodic model ---------\n",
        "class AJMNIST_AxisPeriodic_Compact(nn.Module):\n",
        "    \"\"\"\n",
        "    AJ base (anchored + normalized) + axis-aligned Fourier torus features,\n",
        "    followed by a FIXED orthogonal projector P: R^{D_feat} → R^r and a tiny classifier.\n",
        "\n",
        "    D_feat = 2*(2g)*K (cos/sin per coordinate per harmonic)\n",
        "    \"\"\"\n",
        "    def __init__(self, genus, I_plus, Om_plus, grid_r, grid_i, branch_pts,\n",
        "                 anchors_xy, mu, sigma,\n",
        "                 embed_dim=8, K=2, r=32,\n",
        "                 learnable_freqs=False, proj_seed=1234):\n",
        "        super().__init__()\n",
        "        self.base = AJMNIST_Anchored(\n",
        "            genus, I_plus, Om_plus,\n",
        "            grid_r, grid_i, branch_pts,\n",
        "            anchors_xy, mu, sigma,\n",
        "            embed_dim=embed_dim\n",
        "        )\n",
        "        self.K = K\n",
        "        D = 2*genus\n",
        "        self.torus = TorusFeatures(D, K=K, learnable=learnable_freqs)\n",
        "        D_feat = 2 * D * K   # cos+sin per dim per harmonic\n",
        "        assert r <= D_feat, f\"r={r} must be <= D_feat={D_feat} for K={K}, g={genus}\"\n",
        "        P = make_fixed_orthogonal(D_feat, r, seed=proj_seed)\n",
        "        self.register_buffer(\"P\", P)               # non-trainable projector\n",
        "        self.scale = nn.Parameter(torch.ones(r))   # tiny learned diagonal scale\n",
        "        self.classifier = nn.Linear(r, 10)         # small head\n",
        "\n",
        "    @property\n",
        "    def genus(self):\n",
        "        return self.base.genus\n",
        "\n",
        "    def forward(self, x, return_aux=False):\n",
        "        B = x.size(0)\n",
        "        # shared conv + point head (same as axis model)\n",
        "        h = self.base.conv(x).view(B, -1)\n",
        "        h_exp = h.unsqueeze(1).expand(-1, self.genus, -1)\n",
        "        emb   = self.base.embed.unsqueeze(0).expand(B, -1, -1)\n",
        "        out   = self.base.point_head(torch.cat([h_exp, emb], dim=2)) \\\n",
        "              + self.base.point_bias.unsqueeze(0)\n",
        "        raw_xy, sheet_logits = out[..., :2], out[..., 2]\n",
        "\n",
        "        coords_std, aux = self.base.aj(raw_xy, sheet_logits, return_aux=True)  # (B, 2g)\n",
        "        feats = self.torus(coords_std)                                         # (B, D_feat)\n",
        "        P = self.P.to(feats.device, dtype=feats.dtype)                         # (D_feat, r)\n",
        "        z = feats @ P                                                          # (B, r)\n",
        "        z = z * self.scale                                                     # (B, r)\n",
        "        logits = self.classifier(z)                                            # (B, 10)\n",
        "        if return_aux:\n",
        "            return logits, aux\n",
        "        return logits\n",
        "\n",
        "# --------- param counting helpers ---------\n",
        "def count_params(m):\n",
        "    return sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
        "\n",
        "def heads_only_params(m):\n",
        "    total = count_params(m)\n",
        "    conv_params = sum(p.numel() for p in m.base.conv.parameters() if p.requires_grad)\n",
        "    return total - conv_params\n",
        "\n",
        "# --------- AMP-aware training helpers (reuse if present) ---------\n",
        "# Removed the 'if' guard to ensure function definition is always updated.\n",
        "ce = nn.CrossEntropyLoss()\n",
        "USE_AMP = (device.type == 'cuda')\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n",
        "\n",
        "def train_epoch_amp(model, loader, opt,\n",
        "                    clip=1.0, lam_branch=1e-3, lam_bound=1e-3):\n",
        "    model.train()\n",
        "    tot, correct, n = 0.0, 0, 0\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        with torch.cuda.amp.autocast(enabled=USE_AMP):\n",
        "            logits, aux = model(x, return_aux=True)\n",
        "            loss = ce(logits, y)\n",
        "            loss = loss + lam_branch*aux.get(\"branch_penalty\", 0.0) \\\n",
        "                       + lam_bound *aux.get(\"bound_penalty\",  0.0)\n",
        "        scaler.scale(loss).backward()\n",
        "        if clip is not None:\n",
        "            scaler.unscale_(opt)\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        scaler.step(opt)\n",
        "        scaler.update()\n",
        "        tot     += loss.item() * x.size(0)\n",
        "        correct += (logits.argmax(1) == y).sum().item()\n",
        "        n       += x.size(0)\n",
        "    return tot/n, 100.0*correct/n\n",
        "\n",
        "# Removed the 'if' guard to ensure function definition is always updated.\n",
        "ce_eval = nn.CrossEntropyLoss()\n",
        "@torch.no_grad()\n",
        "def eval_epoch(model, loader):\n",
        "    model.eval()\n",
        "    tot, correct, n = 0.0, 0, 0\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        loss = ce_eval(logits, y)\n",
        "        tot     += loss.item() * x.size(0)\n",
        "        correct += (logits.argmax(1) == y).sum().item()\n",
        "        n += x.size(0)\n",
        "    return tot/n, 100.0*correct/n\n",
        "\n",
        "# Optional AJ diagnostics\n",
        "def maybe_aj_diags(model, loader, n_batches=2):\n",
        "    if \"aj_diags\" in globals():\n",
        "        aj_diags(model, loader, device, n_batches=n_batches)\n",
        "\n",
        "# --------- move lookup tables to device if needed ---------\n",
        "# If you already have device copies, reuse; else create them.\n",
        "I_plus_dev  = I_plus.to(device)   if I_plus.device.type  == 'cpu' else I_plus\n",
        "Om_plus_dev = Om_plus.to(device)  if Om_plus.device.type == 'cpu' else Om_plus\n",
        "grid_r_dev  = grid_r.to(device)\n",
        "grid_i_dev  = grid_i.to(device)\n",
        "branch_pts_dev = branch_pts_t.to(device)\n",
        "anchors_xy_dev = anchors_xy_t.to(device)\n",
        "mu_dev, sigma_dev = mu_t.to(device), sigma_t.to(device)\n",
        "\n",
        "# --------- instantiate compact model ---------\n",
        "K = 2       # harmonics per coordinate (same as axis model)\n",
        "r = 32      # compact dimension (you can tune this)\n",
        "EMBED_DIM = 4\n",
        "\n",
        "aj_axis_compact = AJMNIST_AxisPeriodic_Compact(\n",
        "    genus,\n",
        "    I_plus_dev, Om_plus_dev,\n",
        "    grid_r_dev, grid_i_dev,\n",
        "    branch_pts_dev,\n",
        "    anchors_xy_dev,\n",
        "    mu_dev, sigma_dev,\n",
        "    embed_dim=EMBED_DIM,\n",
        "    K=K, r=r,\n",
        "    learnable_freqs=False,\n",
        "    proj_seed=1234\n",
        ").to(device)\n",
        "\n",
        "total = count_params(aj_axis_compact)\n",
        "heads = heads_only_params(aj_axis_compact)\n",
        "conv  = sum(p.numel() for p in aj_axis_compact.base.conv.parameters() if p.requires_grad)\n",
        "print(f\"\\nAJ axis-periodic COMPACT (g={genus}, K={K}, r={r})\")\n",
        "print(f\"  total params  : {total:,}\")\n",
        "print(f\"  conv trunk    : {conv:,}\")\n",
        "print(f\"  heads-only    : {heads:,} (total minus conv trunk)\")\n",
        "\n",
        "# --------- optimizer (two-tier) ---------\n",
        "fast_params = list(aj_axis_compact.base.point_head.parameters()) + \\\n",
        "              [aj_axis_compact.base.point_bias] + \\\n",
        "              list(aj_axis_compact.torus.parameters()) + \\\n",
        "              list(aj_axis_compact.classifier.parameters())\n",
        "\n",
        "base_params = list(aj_axis_compact.base.conv.parameters()) + \\\n",
        "              list(aj_axis_compact.base.aj.parameters())\n",
        "\n",
        "opt_compact = torch.optim.AdamW(\n",
        "    [\n",
        "        {\"params\": base_params, \"lr\": 3e-4},\n",
        "        {\"params\": fast_params, \"lr\": 1e-3},\n",
        "    ],\n",
        "    weight_decay=1e-4\n",
        ")\n",
        "\n",
        "# --------- train compact model ---------\n",
        "EPOCHS     = 8\n",
        "CLIP_NORM  = 1.0\n",
        "LAM_BRANCH = 1e-3\n",
        "LAM_BOUND  = 1e-3\n",
        "\n",
        "USE_AMP = (device.type == 'cuda')\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n",
        "\n",
        "print(\"\\nStarting training of COMPACT axis-periodic model (AMP =\", USE_AMP, \")\")\n",
        "for ep in range(1, EPOCHS+1):\n",
        "    tr_loss, tr_acc = train_epoch_amp(\n",
        "        aj_axis_compact, train_loader_compact, opt_compact,\n",
        "        clip=CLIP_NORM, lam_branch=LAM_BRANCH, lam_bound=LAM_BOUND\n",
        "    )\n",
        "    te_loss, te_acc = eval_epoch(aj_axis_compact, test_loader_compact)\n",
        "    print(f\"[AJ axis COMPACT K={K}, r={r}] Ep {ep:02d} | \"\n",
        "          f\"train {tr_loss:.4f} / {tr_acc:.2f}% | \"\n",
        "          f\"test {te_loss:.4f} / {te_acc:.2f}%\")\n",
        "    maybe_aj_diags(aj_axis_compact, train_loader_compact, n_batches=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoLajolRctHh",
        "outputId": "80af350c-c0f9-4b65-e92d-ab8d0739183c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Current genus = 30\n",
            "train_loader_compact batch_size: 64\n",
            "\n",
            "AJ axis-periodic COMPACT (g=30, K=2, r=32)\n",
            "  total params  : 20,203\n",
            "  conv trunk    : 18,816\n",
            "  heads-only    : 1,387 (total minus conv trunk)\n",
            "\n",
            "Starting training of COMPACT axis-periodic model (AMP = True )\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3491753158.py:106: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n",
            "/tmp/ipython-input-3491753158.py:211: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n",
            "/tmp/ipython-input-3491753158.py:115: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=USE_AMP):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[AJ axis COMPACT K=2, r=32] Ep 01 | train 1.9336 / 30.23% | test 1.3924 / 48.78%\n",
            "[AJ axis COMPACT K=2, r=32] Ep 02 | train 1.1924 / 54.39% | test 1.1871 / 54.16%\n",
            "[AJ axis COMPACT K=2, r=32] Ep 03 | train 0.9494 / 66.23% | test 1.1348 / 56.43%\n",
            "[AJ axis COMPACT K=2, r=32] Ep 04 | train 0.7695 / 73.50% | test 0.8765 / 67.23%\n",
            "[AJ axis COMPACT K=2, r=32] Ep 05 | train 0.6238 / 79.56% | test 0.5535 / 82.19%\n",
            "[AJ axis COMPACT K=2, r=32] Ep 06 | train 0.5504 / 82.64% | test 0.4156 / 87.85%\n",
            "[AJ axis COMPACT K=2, r=32] Ep 07 | train 0.4398 / 86.82% | test 0.3649 / 88.90%\n",
            "[AJ axis COMPACT K=2, r=32] Ep 08 | train 0.3583 / 89.19% | test 0.3125 / 91.04%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title RFF-Compact baseline (conv → 2 → RFF(2→4gK) → fixed P→r → 10), AMP-safe\n",
        "\n",
        "import torch, math\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# -------- Sanity --------\n",
        "assert 'genus' in globals(), \"Define `genus` first (e.g., genus = 30).\"\n",
        "assert 'train_loader' in globals() and 'test_loader' in globals(), \"Load your dataset loaders first.\"\n",
        "print(f\"Genus = {genus} → 2g = {2*genus}\")\n",
        "\n",
        "# -------- Small-batch loaders (reuse if present) --------\n",
        "BASE_BS = 64\n",
        "orig_train_loader = train_loader\n",
        "orig_test_loader  = test_loader\n",
        "orig_bs = getattr(orig_train_loader, \"batch_size\", None)\n",
        "small_bs = BASE_BS if (orig_bs is None) else min(BASE_BS, orig_bs)\n",
        "\n",
        "train_loader_rff = DataLoader(\n",
        "    orig_train_loader.dataset,\n",
        "    batch_size=small_bs, shuffle=True,\n",
        "    num_workers=getattr(orig_train_loader, \"num_workers\", 0),\n",
        "    pin_memory=getattr(orig_train_loader, \"pin_memory\", False),\n",
        "    drop_last=True\n",
        ")\n",
        "test_loader_rff = DataLoader(\n",
        "    orig_test_loader.dataset,\n",
        "    batch_size=min(256, small_bs*2), shuffle=False,\n",
        "    num_workers=getattr(orig_test_loader, \"num_workers\", 0),\n",
        "    pin_memory=getattr(orig_test_loader, \"pin_memory\", False),\n",
        "    drop_last=False\n",
        ")\n",
        "print(f\"RFF-compact train batch size: {train_loader_rff.batch_size}\")\n",
        "\n",
        "# -------- Helpers --------\n",
        "def make_conv_trunk():\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(1, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
        "        nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
        "        nn.AdaptiveAvgPool2d((1, 1))\n",
        "    )\n",
        "\n",
        "def init_two_band_(tensor, low=1.0, high=3.0):\n",
        "    with torch.no_grad():\n",
        "        sign = (torch.randint(0, 2, tensor.shape, device=tensor.device)*2 - 1).float()\n",
        "        mag  = torch.empty_like(tensor, dtype=torch.float32).uniform_(low, high)\n",
        "        tensor.copy_(sign * mag)\n",
        "\n",
        "def make_fixed_orthogonal(D: int, r: int, seed: int = 1234):\n",
        "    assert r <= D, f\"r={r} must be ≤ D={D}\"\n",
        "    gen = torch.Generator().manual_seed(seed)\n",
        "    A = torch.randn(D, r, generator=gen)\n",
        "    Q, _ = torch.linalg.qr(A, mode='reduced')   # D×r\n",
        "    return Q[:, :r].float()\n",
        "\n",
        "# -------- RFF-Compact model (mirrors AJ-compact head) --------\n",
        "class RFFCompact2DTo2g(nn.Module):\n",
        "    \"\"\"\n",
        "    conv(1→64) → Linear(64→2) → BN(no affine) → α·z\n",
        "      → RFF lift: z ↦ [cos(Wz+b), sin(Wz+b)] with M = 2gK (so D_feat = 2M = 4gK)\n",
        "      → fixed orthonormal projector P: D_feat→r\n",
        "      → diag scale (learnable r)\n",
        "      → Linear(r→10)\n",
        "    All RFF params (W,b) and projector P are frozen.\n",
        "    \"\"\"\n",
        "    def __init__(self, genus: int, K: int = 2, r: int = 32,\n",
        "                 omega_std: float = 1.0, proj_seed: int = 1234, rff_seed: int = 1234):\n",
        "        super().__init__()\n",
        "        self.genus = genus\n",
        "        self.K = K\n",
        "        g = genus\n",
        "\n",
        "        # Shared trunk + 2D bottleneck\n",
        "        self.conv = make_conv_trunk()\n",
        "        self.fc_down = nn.Linear(64, 2)\n",
        "        init_two_band_(self.fc_down.weight); nn.init.zeros_(self.fc_down.bias)\n",
        "\n",
        "        # Stabilize the 2D code numerically (no learnable affine)\n",
        "        self.bn2 = nn.BatchNorm1d(2, affine=False, eps=1e-5, momentum=0.1)\n",
        "        self.alpha = nn.Parameter(torch.tensor(1.0))   # global scale on z\n",
        "\n",
        "        # RFF: choose M so that D_feat matches AJ Fourier head (D_feat = 4 g K)\n",
        "        M = 2 * g * K\n",
        "        self.M = M\n",
        "        gen = torch.Generator().manual_seed(rff_seed)\n",
        "        W = torch.randn(M, 2, generator=gen) * omega_std      # (M,2) rows are ω_i\n",
        "        b = 2*math.pi * torch.rand(M, generator=gen)          # (M,)\n",
        "        self.register_buffer(\"W\", W.float())\n",
        "        self.register_buffer(\"b\", b.float())\n",
        "\n",
        "        # Fixed projector P: D_feat × r, with D_feat = 2M = 4 g K\n",
        "        D_feat = 2 * M\n",
        "        assert r <= D_feat, f\"r={r} must be ≤ D_feat={D_feat} for g={g}, K={K}\"\n",
        "        P = make_fixed_orthogonal(D_feat, r, seed=proj_seed)\n",
        "        self.register_buffer(\"P\", P)                     # (D_feat, r), frozen\n",
        "\n",
        "        # Tiny trainable head on top\n",
        "        self.scale = nn.Parameter(torch.ones(r))         # diag scale after P\n",
        "        self.classifier = nn.Linear(r, 10)\n",
        "\n",
        "    def forward(self, x, return_aux: bool = False):\n",
        "        B = x.size(0)\n",
        "        h = self.conv(x).view(B, -1)                     # (B,64)\n",
        "        z = self.fc_down(h)                              # (B,2)\n",
        "        z = self.bn2(z) * self.alpha                     # (B,2)\n",
        "\n",
        "        # RFF lift: (B,2) → (B, 2M) with cos & sin\n",
        "        # theta = z @ W^T + b\n",
        "        W = self.W.to(z.device, dtype=z.dtype)\n",
        "        b = self.b.to(z.device, dtype=z.dtype)\n",
        "        theta = z @ W.T + b                              # (B, M)\n",
        "        feats = torch.cat([torch.cos(theta), torch.sin(theta)], dim=1)  # (B, 2M)\n",
        "\n",
        "        # Compact head: project to r, scale, classify\n",
        "        P = self.P.to(feats.device, dtype=feats.dtype)   # (2M, r)\n",
        "        zc = (feats @ P) * self.scale                    # (B, r)\n",
        "        logits = self.classifier(zc)                     # (B, 10)\n",
        "\n",
        "        if return_aux:\n",
        "            zero = torch.zeros((), device=x.device)\n",
        "            aux = {\"branch_penalty\": zero, \"bound_penalty\": zero}\n",
        "            return logits, aux\n",
        "        return logits\n",
        "\n",
        "# -------- Parameter accounting --------\n",
        "def count_params(m):\n",
        "    return sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
        "\n",
        "def conv_param_count(m):\n",
        "    return sum(p.numel() for n,p in m.named_parameters()\n",
        "               if p.requires_grad and n.startswith(\"conv\"))\n",
        "\n",
        "def heads_only_params(m):\n",
        "    return count_params(m) - conv_param_count(m)\n",
        "\n",
        "# -------- AMP training helpers --------\n",
        "USE_AMP = (device.type == 'cuda')\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n",
        "ce = nn.CrossEntropyLoss()\n",
        "\n",
        "def train_epoch_amp(model, loader, opt, clip=1.0):\n",
        "    model.train()\n",
        "    tot, correct, n = 0.0, 0, 0\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        with torch.cuda.amp.autocast(enabled=USE_AMP):\n",
        "            logits = model(x)\n",
        "            loss   = ce(logits, y)\n",
        "        scaler.scale(loss).backward()\n",
        "        if clip is not None:\n",
        "            scaler.unscale_(opt)\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        scaler.step(opt); scaler.update()\n",
        "        tot += loss.item() * x.size(0)\n",
        "        correct += (logits.argmax(1) == y).sum().item()\n",
        "        n += x.size(0)\n",
        "    return tot/n, 100.0*correct/n\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_epoch(model, loader):\n",
        "    model.eval()\n",
        "    tot, correct, n = 0.0, 0, 0\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        tot += ce(logits, y).item() * x.size(0)\n",
        "        correct += (logits.argmax(1) == y).sum().item()\n",
        "        n += x.size(0)\n",
        "    return tot/n, 100.0*correct/n\n",
        "\n",
        "# -------- Instantiate & train (match AJ-compact defaults) --------\n",
        "K = 2\n",
        "r = 32\n",
        "OMEGA_STD = 1.0         # try 0.5, 1.0, 2.0; α (learned) will also adapt\n",
        "model = RFFCompact2DTo2g(genus, K=K, r=r, omega_std=OMEGA_STD,\n",
        "                         proj_seed=1234, rff_seed=1234).to(device)\n",
        "\n",
        "total = count_params(model)\n",
        "conv  = conv_param_count(model)\n",
        "heads = heads_only_params(model)\n",
        "print(f\"\\nRFF-Compact (g={genus}, K={K}, r={r})\")\n",
        "print(f\"  total params : {total:,}\")\n",
        "print(f\"  conv trunk   : {conv:,}\")\n",
        "print(f\"  heads-only   : {heads:,}  (total minus conv trunk)\")\n",
        "\n",
        "# Two-tier optimizer, analogous to AJ-compact (base=conv; fast=head)\n",
        "fast_params = list(model.fc_down.parameters()) + [model.alpha, model.scale] + list(model.classifier.parameters())\n",
        "base_params = list(model.conv.parameters())\n",
        "opt = torch.optim.AdamW(\n",
        "    [{\"params\": base_params, \"lr\": 3e-4},\n",
        "     {\"params\": fast_params, \"lr\": 1e-3}],\n",
        "    weight_decay=1e-4\n",
        ")\n",
        "\n",
        "EPOCHS, CLIP = 8, 1.0\n",
        "print(\"\\nStarting training (AMP =\", USE_AMP, \")\")\n",
        "for ep in range(1, EPOCHS+1):\n",
        "    tr_loss, tr_acc = train_epoch_amp(model, train_loader_rff, opt, clip=CLIP)\n",
        "    te_loss, te_acc = eval_epoch(model, test_loader_rff)\n",
        "    print(f\"[RFF-Compact K={K}, r={r}] Ep {ep:02d} | \"\n",
        "          f\"train {tr_loss:.4f}/{tr_acc:.2f}% | test {te_loss:.4f}/{te_acc:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boQbquX5PQKU",
        "outputId": "9ce9c050-ec88-466f-d73f-04749a614d86"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Genus = 30 → 2g = 60\n",
            "RFF-compact train batch size: 64\n",
            "\n",
            "RFF-Compact (g=30, K=2, r=32)\n",
            "  total params : 19,309\n",
            "  conv trunk   : 18,816\n",
            "  heads-only   : 493  (total minus conv trunk)\n",
            "\n",
            "Starting training (AMP = True )\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1910172792.py:142: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n",
            "/tmp/ipython-input-1910172792.py:151: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=USE_AMP):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RFF-Compact K=2, r=32] Ep 01 | train 1.1453/64.04% | test 1.7621/36.55%\n",
            "[RFF-Compact K=2, r=32] Ep 02 | train 0.6380/78.74% | test 1.0487/62.77%\n",
            "[RFF-Compact K=2, r=32] Ep 03 | train 0.5454/81.90% | test 0.7104/74.68%\n",
            "[RFF-Compact K=2, r=32] Ep 04 | train 0.5025/83.34% | test 0.8979/68.43%\n",
            "[RFF-Compact K=2, r=32] Ep 05 | train 0.4711/84.51% | test 0.6324/78.21%\n",
            "[RFF-Compact K=2, r=32] Ep 06 | train 0.4479/85.37% | test 0.5578/80.50%\n",
            "[RFF-Compact K=2, r=32] Ep 07 | train 0.4231/86.23% | test 0.3735/88.41%\n",
            "[RFF-Compact K=2, r=32] Ep 08 | train 0.4104/86.58% | test 0.4425/85.79%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Random 2→2g nonlinear baselines (RFF and Frozen-MLP) — definition + training\n",
        "\n",
        "import torch, math\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device:\", device)\n",
        "assert 'genus' in globals(), \"Please define `genus` first.\"\n",
        "assert 'train_loader' in globals() and 'test_loader' in globals(), \"Please define train/test loaders.\"\n",
        "\n",
        "# -------------------- reuse smaller DataLoaders (like AJ cells) --------------------\n",
        "BASE_BS = 64\n",
        "orig_train_loader = train_loader\n",
        "orig_test_loader  = test_loader\n",
        "orig_bs = getattr(orig_train_loader, \"batch_size\", None)\n",
        "small_bs = BASE_BS if (orig_bs is None) else min(BASE_BS, orig_bs)\n",
        "\n",
        "train_loader_rnd = DataLoader(\n",
        "    orig_train_loader.dataset, batch_size=small_bs, shuffle=True,\n",
        "    num_workers=getattr(orig_train_loader, \"num_workers\", 0),\n",
        "    pin_memory=getattr(orig_train_loader, \"pin_memory\", False),\n",
        "    drop_last=True\n",
        ")\n",
        "test_loader_rnd = DataLoader(\n",
        "    orig_test_loader.dataset, batch_size=min(256, small_bs*2), shuffle=False,\n",
        "    num_workers=getattr(orig_test_loader, \"num_workers\", 0),\n",
        "    pin_memory=getattr(orig_test_loader, \"pin_memory\", False),\n",
        "    drop_last=False\n",
        ")\n",
        "print(f\"Random baseline train batch size: {train_loader_rnd.batch_size}\")\n",
        "\n",
        "# -------------------- shared conv trunk (same as AJ baselines) --------------------\n",
        "def make_conv_trunk():\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(1, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
        "        nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
        "        nn.AdaptiveAvgPool2d((1, 1))  # → (B,64,1,1)\n",
        "    )\n",
        "\n",
        "def init_two_band_(tensor, low=1.0, high=3.0):\n",
        "    with torch.no_grad():\n",
        "        sign = (torch.randint(0, 2, tensor.shape, device=tensor.device)*2 - 1).float()\n",
        "        mag  = torch.empty_like(tensor, dtype=torch.float32).uniform_(low, high)\n",
        "        tensor.copy_(sign * mag)\n",
        "\n",
        "# -------------------- Random Fourier Features 2→2g (frozen) --------------------\n",
        "class RandomFourier2Dto2g(nn.Module):\n",
        "    \"\"\"\n",
        "    z ∈ R^2  →  [cos(W z + b), sin(W z + b)] ∈ R^{2g}\n",
        "    W ∈ R^{g×2}, b ∈ R^g are frozen buffers.\n",
        "    \"\"\"\n",
        "    def __init__(self, genus: int, freq_scale: float = 1.0, seed: int = 1234, learn_gain: bool = False):\n",
        "        super().__init__()\n",
        "        self.genus = genus\n",
        "        g = genus\n",
        "        # Frozen random frequencies & phases\n",
        "        gen = torch.Generator().manual_seed(seed)\n",
        "        W = torch.randn(g, 2, generator=gen) * freq_scale\n",
        "        b = 2*math.pi * torch.rand(g, generator=gen)\n",
        "        self.register_buffer(\"W\", W.float())\n",
        "        self.register_buffer(\"b\", b.float())\n",
        "        # Optional learnable per-feature gain (small, to keep fairness)\n",
        "        self.gain = nn.Parameter(torch.ones(2*g)) if learn_gain else None\n",
        "\n",
        "    def forward(self, z):  # z: (B,2)\n",
        "        theta = z @ self.W.T + self.b           # (B,g)\n",
        "        feats = torch.cat([torch.cos(theta), torch.sin(theta)], dim=1)  # (B, 2g)\n",
        "        if self.gain is not None:\n",
        "            feats = feats * self.gain\n",
        "        return feats\n",
        "\n",
        "# -------------------- Frozen random MLP 2→2g --------------------\n",
        "class FrozenMLP2Dto2g(nn.Module):\n",
        "    \"\"\"\n",
        "    z ∈ R^2 → h → 2g with a GELU, weights frozen at init.\n",
        "    \"\"\"\n",
        "    def __init__(self, genus: int, hidden: int = None, seed: int = 1234):\n",
        "        super().__init__()\n",
        "        g = genus\n",
        "        D = 2*g\n",
        "        if hidden is None:\n",
        "            hidden = min(4*g, 128)  # modest hidden width by default\n",
        "        self.fc1 = nn.Linear(2, hidden)\n",
        "        self.fc2 = nn.Linear(hidden, D)\n",
        "        # Init & freeze\n",
        "        torch.manual_seed(seed)\n",
        "        nn.init.xavier_uniform_(self.fc1.weight); nn.init.zeros_(self.fc1.bias)\n",
        "        nn.init.xavier_uniform_(self.fc2.weight); nn.init.zeros_(self.fc2.bias)\n",
        "        for p in self.fc1.parameters(): p.requires_grad = False\n",
        "        for p in self.fc2.parameters(): p.requires_grad = False\n",
        "        self.act = nn.GELU()\n",
        "\n",
        "    def forward(self, z):  # z: (B,2)\n",
        "        h  = self.act(self.fc1(z))\n",
        "        y  = self.fc2(h)              # (B,2g)\n",
        "        return y\n",
        "\n",
        "# -------------------- Projection2D → RandomNonlin(2→2g) → 10 --------------------\n",
        "class Projection2D_RandomLift(nn.Module):\n",
        "    \"\"\"\n",
        "    conv(1→64) → Linear(64→2) → RandomLift(2→2g) [frozen] → Linear(2g→10)\n",
        "    \"\"\"\n",
        "    def __init__(self, genus: int, lift_kind: str = \"rff\", **lift_kwargs):\n",
        "        super().__init__()\n",
        "        self.genus = genus\n",
        "        self.conv = make_conv_trunk()\n",
        "        self.fc_down = nn.Linear(64, 2)\n",
        "        # Lift: choose 'rff' or 'mlp'\n",
        "        if lift_kind == \"rff\":\n",
        "            self.lift = RandomFourier2Dto2g(genus, **lift_kwargs)\n",
        "        elif lift_kind == \"mlp\":\n",
        "            self.lift = FrozenMLP2Dto2g(genus, **lift_kwargs)\n",
        "        else:\n",
        "            raise ValueError(\"lift_kind must be 'rff' or 'mlp'\")\n",
        "        self.classifier = nn.Linear(2*genus, 10)\n",
        "        # init similar to earlier baselines\n",
        "        init_two_band_(self.fc_down.weight); nn.init.zeros_(self.fc_down.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.size(0)\n",
        "        h  = self.conv(x).view(B, -1)   # (B,64)\n",
        "        z2 = self.fc_down(h)            # (B,2)\n",
        "        z  = self.lift(z2)              # (B,2g) frozen nonlinear lift\n",
        "        logits = self.classifier(z)     # (B,10)\n",
        "        return logits\n",
        "\n",
        "# -------------------- Training helpers (AMP) --------------------\n",
        "def count_trainable_params(m):\n",
        "    return sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_epoch(model, loader):\n",
        "    model.eval()\n",
        "    ce = nn.CrossEntropyLoss()\n",
        "    tot, correct, n = 0.0, 0, 0\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        loss = ce(logits, y)\n",
        "        tot     += loss.item() * x.size(0)\n",
        "        correct += (logits.argmax(1) == y).sum().item()\n",
        "        n += x.size(0)\n",
        "    return tot/n, 100.0*correct/n\n",
        "\n",
        "USE_AMP = (device.type == \"cuda\")\n",
        "scaler  = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n",
        "\n",
        "def train_epoch_amp(model, loader, opt, clip=1.0):\n",
        "    model.train()\n",
        "    ce = nn.CrossEntropyLoss()\n",
        "    tot, correct, n = 0.0, 0, 0\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        with torch.cuda.amp.autocast(enabled=USE_AMP):\n",
        "            logits = model(x)\n",
        "            loss   = ce(logits, y)\n",
        "        scaler.scale(loss).backward()\n",
        "        if clip is not None:\n",
        "            scaler.unscale_(opt)\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        scaler.step(opt)\n",
        "        scaler.update()\n",
        "        tot     += loss.item() * x.size(0)\n",
        "        correct += (logits.argmax(1) == y).sum().item()\n",
        "        n       += x.size(0)\n",
        "    return tot/n, 100.0*correct/n\n",
        "\n",
        "# -------------------- Instantiate & train: choose your lift --------------------\n",
        "LIFT_KIND   = \"rff\"     # \"rff\" or \"mlp\"\n",
        "RFF_SCALE   = 1.0       # typical 0.5–2.0; affects oscillation rate of cos/sin\n",
        "RFF_GAIN    = False     # set True to allow a tiny learnable per-feature scale\n",
        "MLP_HIDDEN  = None      # if LIFT_KIND=\"mlp\", None→min(4g,128)\n",
        "EPOCHS      = 8\n",
        "LR          = 3e-4\n",
        "WDECAY      = 1e-4\n",
        "CLIP_NORM   = 1.0\n",
        "\n",
        "if LIFT_KIND == \"rff\":\n",
        "    model = Projection2D_RandomLift(genus, lift_kind=\"rff\", freq_scale=RFF_SCALE, learn_gain=RFF_GAIN).to(device)\n",
        "else:\n",
        "    model = Projection2D_RandomLift(genus, lift_kind=\"mlp\", hidden=MLP_HIDDEN).to(device)\n",
        "\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WDECAY)\n",
        "\n",
        "trainable = count_trainable_params(model)\n",
        "conv_params = sum(p.numel() for n,p in model.named_parameters()\n",
        "                  if p.requires_grad and n.startswith(\"conv\"))\n",
        "heads_only = trainable - conv_params\n",
        "print(f\"\\nRandom 2→2g baseline ({LIFT_KIND}) — trainable params: {trainable:,}\")\n",
        "print(f\"  conv trunk (trainable) : {conv_params:,}\")\n",
        "print(f\"  heads-only (trainable) : {heads_only:,} (total minus conv trunk)\")\n",
        "\n",
        "print(\"\\nStarting training (AMP =\", USE_AMP, \")\")\n",
        "for ep in range(1, EPOCHS+1):\n",
        "    tr_loss, tr_acc = train_epoch_amp(model, train_loader_rnd, opt, clip=CLIP_NORM)\n",
        "    te_loss, te_acc = eval_epoch(model, test_loader_rnd)\n",
        "    print(f\"[RANDOM {LIFT_KIND} g={genus}] Ep {ep:02d} | train {tr_loss:.4f}/{tr_acc:.2f}% | test {te_loss:.4f}/{te_acc:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gz750ss5DyV7",
        "outputId": "e18a0067-a33c-4562-d0d9-49db0ab72bca"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Random baseline train batch size: 64\n",
            "\n",
            "Random 2→2g baseline (rff) — trainable params: 19,556\n",
            "  conv trunk (trainable) : 18,816\n",
            "  heads-only (trainable) : 740 (total minus conv trunk)\n",
            "\n",
            "Starting training (AMP = True )\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3403247466.py:148: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler  = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n",
            "/tmp/ipython-input-3403247466.py:157: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=USE_AMP):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RANDOM rff g=30] Ep 01 | train 1.5800/46.61% | test 1.2498/56.25%\n",
            "[RANDOM rff g=30] Ep 02 | train 1.0110/66.91% | test 0.8972/68.53%\n",
            "[RANDOM rff g=30] Ep 03 | train 0.8105/72.76% | test 0.6477/79.20%\n",
            "[RANDOM rff g=30] Ep 04 | train 0.6922/76.75% | test 0.9236/65.13%\n",
            "[RANDOM rff g=30] Ep 05 | train 0.6384/78.38% | test 0.6232/77.96%\n",
            "[RANDOM rff g=30] Ep 06 | train 0.6169/79.11% | test 0.5482/82.09%\n",
            "[RANDOM rff g=30] Ep 07 | train 0.5643/81.17% | test 0.4730/84.76%\n",
            "[RANDOM rff g=30] Ep 08 | train 0.5459/81.88% | test 0.5397/82.07%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Random 2→2g nonlinear baselines (RFF and Frozen-MLP) — definition + training\n",
        "\n",
        "import torch, math\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device:\", device)\n",
        "assert 'genus' in globals(), \"Please define `genus` first.\"\n",
        "assert 'train_loader' in globals() and 'test_loader' in globals(), \"Please define train/test loaders.\"\n",
        "\n",
        "# -------------------- reuse smaller DataLoaders (like AJ cells) --------------------\n",
        "BASE_BS = 64\n",
        "orig_train_loader = train_loader\n",
        "orig_test_loader  = test_loader\n",
        "orig_bs = getattr(orig_train_loader, \"batch_size\", None)\n",
        "small_bs = BASE_BS if (orig_bs is None) else min(BASE_BS, orig_bs)\n",
        "\n",
        "train_loader_rnd = DataLoader(\n",
        "    orig_train_loader.dataset, batch_size=small_bs, shuffle=True,\n",
        "    num_workers=getattr(orig_train_loader, \"num_workers\", 0),\n",
        "    pin_memory=getattr(orig_train_loader, \"pin_memory\", False),\n",
        "    drop_last=True\n",
        ")\n",
        "test_loader_rnd = DataLoader(\n",
        "    orig_test_loader.dataset, batch_size=min(256, small_bs*2), shuffle=False,\n",
        "    num_workers=getattr(orig_test_loader, \"num_workers\", 0),\n",
        "    pin_memory=getattr(orig_test_loader, \"pin_memory\", False),\n",
        "    drop_last=False\n",
        ")\n",
        "print(f\"Random baseline train batch size: {train_loader_rnd.batch_size}\")\n",
        "\n",
        "# -------------------- shared conv trunk (same as AJ baselines) --------------------\n",
        "def make_conv_trunk():\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(1, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
        "        nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
        "        nn.AdaptiveAvgPool2d((1, 1))  # → (B,64,1,1)\n",
        "    )\n",
        "\n",
        "def init_two_band_(tensor, low=1.0, high=3.0):\n",
        "    with torch.no_grad():\n",
        "        sign = (torch.randint(0, 2, tensor.shape, device=tensor.device)*2 - 1).float()\n",
        "        mag  = torch.empty_like(tensor, dtype=torch.float32).uniform_(low, high)\n",
        "        tensor.copy_(sign * mag)\n",
        "\n",
        "# -------------------- Random Fourier Features 2→2g (frozen) --------------------\n",
        "class RandomFourier2Dto2g(nn.Module):\n",
        "    \"\"\"\n",
        "    z ∈ R^2  →  [cos(W z + b), sin(W z + b)] ∈ R^{2g}\n",
        "    W ∈ R^{g×2}, b ∈ R^g are frozen buffers.\n",
        "    \"\"\"\n",
        "    def __init__(self, genus: int, freq_scale: float = 1.0, seed: int = 1234, learn_gain: bool = False):\n",
        "        super().__init__()\n",
        "        self.genus = genus\n",
        "        g = genus\n",
        "        # Frozen random frequencies & phases\n",
        "        gen = torch.Generator().manual_seed(seed)\n",
        "        W = torch.randn(g, 2, generator=gen) * freq_scale\n",
        "        b = 2*math.pi * torch.rand(g, generator=gen)\n",
        "        self.register_buffer(\"W\", W.float())\n",
        "        self.register_buffer(\"b\", b.float())\n",
        "        # Optional learnable per-feature gain (small, to keep fairness)\n",
        "        self.gain = nn.Parameter(torch.ones(2*g)) if learn_gain else None\n",
        "\n",
        "    def forward(self, z):  # z: (B,2)\n",
        "        theta = z @ self.W.T + self.b           # (B,g)\n",
        "        feats = torch.cat([torch.cos(theta), torch.sin(theta)], dim=1)  # (B, 2g)\n",
        "        if self.gain is not None:\n",
        "            feats = feats * self.gain\n",
        "        return feats\n",
        "\n",
        "# -------------------- Frozen random MLP 2→2g --------------------\n",
        "class FrozenMLP2Dto2g(nn.Module):\n",
        "    \"\"\"\n",
        "    z ∈ R^2 → h → 2g with a GELU, weights frozen at init.\n",
        "    \"\"\"\n",
        "    def __init__(self, genus: int, hidden: int = None, seed: int = 1234):\n",
        "        super().__init__()\n",
        "        g = genus\n",
        "        D = 2*g\n",
        "        if hidden is None:\n",
        "            hidden = min(4*g, 128)  # modest hidden width by default\n",
        "        self.fc1 = nn.Linear(2, hidden)\n",
        "        self.fc2 = nn.Linear(hidden, D)\n",
        "        # Init & freeze\n",
        "        torch.manual_seed(seed)\n",
        "        nn.init.xavier_uniform_(self.fc1.weight); nn.init.zeros_(self.fc1.bias)\n",
        "        nn.init.xavier_uniform_(self.fc2.weight); nn.init.zeros_(self.fc2.bias)\n",
        "        for p in self.fc1.parameters(): p.requires_grad = False\n",
        "        for p in self.fc2.parameters(): p.requires_grad = False\n",
        "        self.act = nn.GELU()\n",
        "\n",
        "    def forward(self, z):  # z: (B,2)\n",
        "        h  = self.act(self.fc1(z))\n",
        "        y  = self.fc2(h)              # (B,2g)\n",
        "        return y\n",
        "\n",
        "# -------------------- Projection2D → RandomNonlin(2→2g) → 10 --------------------\n",
        "class Projection2D_RandomLift(nn.Module):\n",
        "    \"\"\"\n",
        "    conv(1→64) → Linear(64→2) → RandomLift(2→2g) [frozen] → Linear(2g→10)\n",
        "    \"\"\"\n",
        "    def __init__(self, genus: int, lift_kind: str = \"rff\", **lift_kwargs):\n",
        "        super().__init__()\n",
        "        self.genus = genus\n",
        "        self.conv = make_conv_trunk()\n",
        "        self.fc_down = nn.Linear(64, 2)\n",
        "        # Lift: choose 'rff' or 'mlp'\n",
        "        if lift_kind == \"rff\":\n",
        "            self.lift = RandomFourier2Dto2g(genus, **lift_kwargs)\n",
        "        elif lift_kind == \"mlp\":\n",
        "            self.lift = FrozenMLP2Dto2g(genus, **lift_kwargs)\n",
        "        else:\n",
        "            raise ValueError(\"lift_kind must be 'rff' or 'mlp'\")\n",
        "        self.classifier = nn.Linear(2*genus, 10)\n",
        "        # init similar to earlier baselines\n",
        "        init_two_band_(self.fc_down.weight); nn.init.zeros_(self.fc_down.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.size(0)\n",
        "        h  = self.conv(x).view(B, -1)   # (B,64)\n",
        "        z2 = self.fc_down(h)            # (B,2)\n",
        "        z  = self.lift(z2)              # (B,2g) frozen nonlinear lift\n",
        "        logits = self.classifier(z)     # (B,10)\n",
        "        return logits\n",
        "\n",
        "# -------------------- Training helpers (AMP) --------------------\n",
        "def count_trainable_params(m):\n",
        "    return sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_epoch(model, loader):\n",
        "    model.eval()\n",
        "    ce = nn.CrossEntropyLoss()\n",
        "    tot, correct, n = 0.0, 0, 0\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        loss = ce(logits, y)\n",
        "        tot     += loss.item() * x.size(0)\n",
        "        correct += (logits.argmax(1) == y).sum().item()\n",
        "        n += x.size(0)\n",
        "    return tot/n, 100.0*correct/n\n",
        "\n",
        "USE_AMP = (device.type == \"cuda\")\n",
        "scaler  = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n",
        "\n",
        "def train_epoch_amp(model, loader, opt, clip=1.0):\n",
        "    model.train()\n",
        "    ce = nn.CrossEntropyLoss()\n",
        "    tot, correct, n = 0.0, 0, 0\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        with torch.cuda.amp.autocast(enabled=USE_AMP):\n",
        "            logits = model(x)\n",
        "            loss   = ce(logits, y)\n",
        "        scaler.scale(loss).backward()\n",
        "        if clip is not None:\n",
        "            scaler.unscale_(opt)\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        scaler.step(opt)\n",
        "        scaler.update()\n",
        "        tot     += loss.item() * x.size(0)\n",
        "        correct += (logits.argmax(1) == y).sum().item()\n",
        "        n       += x.size(0)\n",
        "    return tot/n, 100.0*correct/n\n",
        "\n",
        "# -------------------- Instantiate & train: choose your lift --------------------\n",
        "LIFT_KIND   = \"mlp\"     # \"rff\" or \"mlp\"\n",
        "RFF_SCALE   = 1.0       # typical 0.5–2.0; affects oscillation rate of cos/sin\n",
        "RFF_GAIN    = False     # set True to allow a tiny learnable per-feature scale\n",
        "MLP_HIDDEN  = None      # if LIFT_KIND=\"mlp\", None→min(4g,128)\n",
        "EPOCHS      = 8\n",
        "LR          = 3e-4\n",
        "WDECAY      = 1e-4\n",
        "CLIP_NORM   = 1.0\n",
        "\n",
        "if LIFT_KIND == \"rff\":\n",
        "    model = Projection2D_RandomLift(genus, lift_kind=\"rff\", freq_scale=RFF_SCALE, learn_gain=RFF_GAIN).to(device)\n",
        "else:\n",
        "    model = Projection2D_RandomLift(genus, lift_kind=\"mlp\", hidden=MLP_HIDDEN).to(device)\n",
        "\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WDECAY)\n",
        "\n",
        "trainable = count_trainable_params(model)\n",
        "conv_params = sum(p.numel() for n,p in model.named_parameters()\n",
        "                  if p.requires_grad and n.startswith(\"conv\"))\n",
        "heads_only = trainable - conv_params\n",
        "print(f\"\\nRandom 2→2g baseline ({LIFT_KIND}) — trainable params: {trainable:,}\")\n",
        "print(f\"  conv trunk (trainable) : {conv_params:,}\")\n",
        "print(f\"  heads-only (trainable) : {heads_only:,} (total minus conv trunk)\")\n",
        "\n",
        "print(\"\\nStarting training (AMP =\", USE_AMP, \")\")\n",
        "for ep in range(1, EPOCHS+1):\n",
        "    tr_loss, tr_acc = train_epoch_amp(model, train_loader_rnd, opt, clip=CLIP_NORM)\n",
        "    te_loss, te_acc = eval_epoch(model, test_loader_rnd)\n",
        "    print(f\"[RANDOM {LIFT_KIND} g={genus}] Ep {ep:02d} | train {tr_loss:.4f}/{tr_acc:.2f}% | test {te_loss:.4f}/{te_acc:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIOHJujxLDVf",
        "outputId": "4cc74bcd-47ce-4456-8569-784ab83b8990"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Random baseline train batch size: 64\n",
            "\n",
            "Random 2→2g baseline (mlp) — trainable params: 19,556\n",
            "  conv trunk (trainable) : 18,816\n",
            "  heads-only (trainable) : 740 (total minus conv trunk)\n",
            "\n",
            "Starting training (AMP = True )\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3299940884.py:148: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler  = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n",
            "/tmp/ipython-input-3299940884.py:157: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=USE_AMP):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RANDOM mlp g=30] Ep 01 | train 1.5637/37.27% | test 1.3217/46.85%\n",
            "[RANDOM mlp g=30] Ep 02 | train 1.2484/49.17% | test 1.1402/54.48%\n",
            "[RANDOM mlp g=30] Ep 03 | train 1.1176/55.33% | test 1.1090/55.60%\n",
            "[RANDOM mlp g=30] Ep 04 | train 1.0324/59.58% | test 0.9748/62.13%\n",
            "[RANDOM mlp g=30] Ep 05 | train 0.9725/62.51% | test 0.9350/63.36%\n",
            "[RANDOM mlp g=30] Ep 06 | train 0.9164/65.46% | test 0.8966/65.77%\n",
            "[RANDOM mlp g=30] Ep 07 | train 0.8798/66.94% | test 0.8737/67.14%\n",
            "[RANDOM mlp g=30] Ep 08 | train 0.8349/69.60% | test 0.8003/71.42%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-qr2CuKqkT5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sCu9KgRekT1O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}