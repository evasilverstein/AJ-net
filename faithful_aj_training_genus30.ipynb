{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daf755f7",
   "metadata": {},
   "source": [
    "# Faithful Abel–Jacobi (AJ) Training Notebook (Two Sheets)\n",
    "\n",
    "This notebook updates your genus-30 AJ MNIST training pipeline to use the **faithful two-sheet** lookup tables:\n",
    "\n",
    "- `I0(z)` = AJ integrals to the lift on sheet 0  \n",
    "- `I1(z)` = AJ integrals to the lift on sheet 1  \n",
    "- `B`     = constant bridge vector (used in table generation)\n",
    "\n",
    "**Main change:** we replace the old “single-table + learned sign” sheet kludge with a **two-table sheet selector**:\n",
    "- `sign = tanh(sheet_logits)` (keeps your initialization trick)\n",
    "- `w0 = (1+sign)/2`  \n",
    "- `I = w0 * I0 + (1-w0) * I1`\n",
    "\n",
    "Everything else stays in the same spirit: anchored point initialization, per-channel normalization, learnable global gain `gamma`, boundary/branch penalties, and axis-aligned Fourier features (mocking periods).\n",
    "\n",
    "**If you don’t have an ω-table** (`aj_omegas_genus30.pt`), the notebook will compute a fast **ω-magnitude proxy** from the branch points for anchor selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07391f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Setup: installs, imports, Drive mount, paths\n",
    "!pip install -q torch torchvision numpy\n",
    "\n",
    "import os, math, time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from google.colab import drive\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# ===========================\n",
    "# Drive + Paths (match lookup-table notebook)\n",
    "# ===========================\n",
    "DRIVE_FOLDER = \"AJ_Tables_g30\"   # <-- change if you used a different folder\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "SAVE_DIR = f\"/content/drive/MyDrive/{DRIVE_FOLDER}\"\n",
    "\n",
    "# Faithful integrals (I0/I1/B) file:\n",
    "INTEGRALS_PATH = os.path.join(SAVE_DIR, \"aj_integrals_genus30.pt\")\n",
    "\n",
    "# Optional omegas file (only used to compute ω-aware anchors):\n",
    "OMEGAS_PATH    = os.path.join(SAVE_DIR, \"aj_omegas_genus30.pt\")\n",
    "\n",
    "print(\"SAVE_DIR        :\", SAVE_DIR)\n",
    "print(\"INTEGRALS_PATH  :\", INTEGRALS_PATH)\n",
    "print(\"OMEGAS_PATH     :\", OMEGAS_PATH)\n",
    "assert os.path.exists(INTEGRALS_PATH), \"Integrals file not found in Drive.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c4145c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Load faithful tables: I0, I1, B (+ geometry)\n",
    "ints = torch.load(INTEGRALS_PATH, map_location='cpu', weights_only=False)\n",
    "\n",
    "# Required metadata\n",
    "grid_r_np  = np.array(ints[\"grid_r\"])\n",
    "grid_i_np  = np.array(ints[\"grid_i\"])\n",
    "branch_pts = np.array(ints[\"branch_pts\"])\n",
    "branch_cuts = ints.get(\"branch_cuts\", None)\n",
    "\n",
    "# Faithful integrals\n",
    "I0 = ints.get(\"I0\", ints.get(\"I_plus\", None))\n",
    "I1 = ints.get(\"I1\", ints.get(\"I_minus\", None))\n",
    "B  = ints.get(\"B\", None)\n",
    "\n",
    "assert I0 is not None, \"Expected I0 (faithful) or at least I_plus in integrals file.\"\n",
    "\n",
    "# Ensure torch tensors and complex dtype\n",
    "if not torch.is_tensor(I0):\n",
    "    I0 = torch.tensor(I0)\n",
    "I0 = I0.to(torch.cfloat)\n",
    "\n",
    "if I1 is None:\n",
    "    assert B is not None, \"I1 missing and B missing; cannot construct second sheet.\"\n",
    "    if not torch.is_tensor(B):\n",
    "        B = torch.tensor(B)\n",
    "    B = B.to(torch.cfloat)\n",
    "    I1 = B[:, None, None] - I0\n",
    "else:\n",
    "    if not torch.is_tensor(I1):\n",
    "        I1 = torch.tensor(I1)\n",
    "    I1 = I1.to(torch.cfloat)\n",
    "\n",
    "# Ensure B exists (estimate if missing)\n",
    "if B is None:\n",
    "    B = (I0 + I1).mean(dim=(1,2))\n",
    "    print(\"B not found in file → estimated B as mean(I0+I1) over grid.\")\n",
    "else:\n",
    "    if not torch.is_tensor(B):\n",
    "        B = torch.tensor(B)\n",
    "    B = B.to(torch.cfloat)\n",
    "\n",
    "genus = int(ints.get(\"genus\", I0.shape[0]))\n",
    "H, W = I0.shape[-2:]\n",
    "\n",
    "# Torch versions of axes / branch points\n",
    "grid_r = torch.tensor(grid_r_np, dtype=torch.float32)\n",
    "grid_i = torch.tensor(grid_i_np, dtype=torch.float32)\n",
    "branch_pts_t = torch.tensor(branch_pts)  # complex\n",
    "\n",
    "print(f\"Loaded: genus={genus}, grid=({H}x{W})\")\n",
    "print(\"Integrals keys present:\", [k for k in [\"I0\",\"I1\",\"B\",\"I_plus\",\"I_minus\",\"sheet_parity\",\"branch_cuts\"] if k in ints])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fef312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Optional: load ω-table (if you generated/saved it)\n",
    "Om_plus = None\n",
    "if os.path.exists(OMEGAS_PATH):\n",
    "    omeg = torch.load(OMEGAS_PATH, map_location='cpu', weights_only=False)\n",
    "    Om_plus = omeg.get(\"omega_plus\", None)\n",
    "    if Om_plus is not None:\n",
    "        Om_plus = Om_plus.to(torch.cfloat)\n",
    "        print(\"Loaded omega_plus:\", tuple(Om_plus.shape))\n",
    "    else:\n",
    "        print(\"OMEGAS_PATH exists but no key 'omega_plus' found; will use proxy ω-map.\")\n",
    "else:\n",
    "    print(\"No omegas file found; will use proxy ω-map from branch points.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e45885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Compute ω-aware anchors and AJ normalization (mu/sigma)\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# -----------------------------\n",
    "# 1) ω-strength map for anchors\n",
    "# -----------------------------\n",
    "with torch.no_grad():\n",
    "    if Om_plus is not None:\n",
    "        Wmap = Om_plus.abs().sum(dim=0)          # (H,W)\n",
    "        Wmap_np = Wmap.cpu().numpy()\n",
    "        print(\"Anchor map: using ω-table (sum_k |ω_k|).\")\n",
    "    else:\n",
    "        # Proxy (magnitude-only): score(z) = log sum_k |z|^k / |sqrt(P(z))|\n",
    "        # Enough for choosing anchors (we only need a monotone proxy).\n",
    "        print(\"Anchor map: using proxy score from branch points (no ω-table).\")\n",
    "\n",
    "        X = torch.tensor(grid_r_np[None, :].repeat(H, axis=0), dtype=torch.float64)\n",
    "        Y = torch.tensor(grid_i_np[:, None].repeat(W, axis=1), dtype=torch.float64)\n",
    "\n",
    "        bp = torch.tensor(branch_pts, dtype=torch.complex128)\n",
    "        bp_real = bp.real.view(1,1,-1).to(torch.float64)\n",
    "        bp_imag = bp.imag.view(1,1,-1).to(torch.float64)\n",
    "\n",
    "        dx = X.unsqueeze(-1) - bp_real\n",
    "        dy = Y.unsqueeze(-1) - bp_imag\n",
    "        d  = torch.sqrt(dx*dx + dy*dy).clamp_min(1e-12)\n",
    "        log_absP = torch.log(d).sum(dim=-1)\n",
    "        log_abs_sqrtP = 0.5 * log_absP\n",
    "\n",
    "        absZ = torch.sqrt(X*X + Y*Y).clamp_min(1e-12)\n",
    "        log_absZ = torch.log(absZ)\n",
    "\n",
    "        k = torch.arange(genus, dtype=torch.float64).view(1,1,-1)\n",
    "        terms = k * log_absZ.unsqueeze(-1) - log_abs_sqrtP.unsqueeze(-1)\n",
    "        Wmap_log = torch.logsumexp(terms, dim=-1)    # (H,W)\n",
    "        Wmap_np = Wmap_log.cpu().numpy()\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Mask edges + avoid branch points\n",
    "# -----------------------------\n",
    "gy = np.linspace(-1, 1, H)[:, None]\n",
    "gx = np.linspace(-1, 1, W)[None, :]\n",
    "edge_mask = (np.abs(gx) < 0.92) & (np.abs(gy) < 0.92)\n",
    "\n",
    "grid_x = grid_r_np[None, :].repeat(H, axis=0)\n",
    "grid_y = grid_i_np[:, None].repeat(W, axis=1)\n",
    "\n",
    "bp_real = np.real(branch_pts).reshape(-1, 1, 1)\n",
    "bp_imag = np.imag(branch_pts).reshape(-1, 1, 1)\n",
    "d2 = (grid_x - bp_real)**2 + (grid_y - bp_imag)**2\n",
    "bp_mask = (d2.min(axis=0) > 0.25)     # keep points with dist > ~0.5\n",
    "\n",
    "mask = edge_mask & bp_mask\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Candidate set = top quantile by ω-strength\n",
    "# -----------------------------\n",
    "score = np.where(mask, Wmap_np, -np.inf)\n",
    "valid = score[score > -np.inf]\n",
    "assert valid.size > 0, \"Mask removed all points; relax edge/bp masks.\"\n",
    "\n",
    "q = 0.85\n",
    "cand = np.argwhere(score >= np.quantile(valid, q))\n",
    "while cand.shape[0] < genus and q > 0.50:\n",
    "    q -= 0.05\n",
    "    cand = np.argwhere(score >= np.quantile(valid, q))\n",
    "\n",
    "print(f\"Anchor candidates: {cand.shape[0]} points (quantile={q:.2f})\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Farthest-point sampling for diverse anchors\n",
    "# -----------------------------\n",
    "def farthest_k(points_hw, k):\n",
    "    pts = points_hw.copy()\n",
    "    start = pts[np.argmax(score[tuple(pts.T)])]\n",
    "    chosen = [start]\n",
    "    if k == 1:\n",
    "        return np.array(chosen)\n",
    "\n",
    "    coords = np.stack([grid_x[tuple(pts.T)], grid_y[tuple(pts.T)]], axis=1)  # (N,2)\n",
    "    c0 = np.array([grid_x[start[0], start[1]], grid_y[start[0], start[1]]])[None, :]\n",
    "    mind = np.sum((coords - c0)**2, axis=1)\n",
    "\n",
    "    for _ in range(1, k):\n",
    "        j = np.argmax(mind)\n",
    "        chosen.append(pts[j])\n",
    "        cj = coords[j][None, :]\n",
    "        mind = np.minimum(mind, np.sum((coords - cj)**2, axis=1))\n",
    "    return np.array(chosen)\n",
    "\n",
    "anchors_hw = farthest_k(cand, genus)  # (g,2) with (iy,ix)\n",
    "x0 = grid_r_np[anchors_hw[:, 1]]\n",
    "y0 = grid_i_np[anchors_hw[:, 0]]\n",
    "anchors_xy = np.stack([x0, y0], axis=1)          # (g,2)\n",
    "anchors_xy_t = torch.tensor(anchors_xy, dtype=torch.float32)\n",
    "\n",
    "print(\"Example anchors (first 8):\")\n",
    "for i in range(min(8, genus)):\n",
    "    print(f\"  {i:02d}: x0={anchors_xy[i,0]:+.3f}, y0={anchors_xy[i,1]:+.3f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 5) AJ normalization stats (use BOTH sheets)\n",
    "# -----------------------------\n",
    "I0_ch = torch.cat([I0.real, I0.imag], dim=0)   # (2g,H,W)\n",
    "I1_ch = torch.cat([I1.real, I1.imag], dim=0)   # (2g,H,W)\n",
    "I_cat = torch.stack([I0_ch, I1_ch], dim=0)     # (2,2g,H,W)\n",
    "\n",
    "mu = I_cat.mean(dim=(0,2,3))                   # (2g,)\n",
    "sigma = I_cat.std(dim=(0,2,3)).clamp_min(1e-6) # (2g,)\n",
    "\n",
    "mu_t = mu.float()\n",
    "sigma_t = sigma.float()\n",
    "\n",
    "print(\"mu/sigma computed from both sheets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968b2d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title AJ core: faithful two-sheet lookup + normalization + penalties\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def _pack_complex_table(table_gHW: torch.Tensor) -> torch.Tensor:\n",
    "    # Input:  (g,H,W) complex\n",
    "    # Output: (1,2g,H,W) real with channels [Re..., Im...]\n",
    "    re, im = table_gHW.real, table_gHW.imag\n",
    "    return torch.cat([re, im], dim=0).unsqueeze(0).contiguous()\n",
    "\n",
    "class AJGridActivationNorm(nn.Module):\n",
    "    def __init__(self,\n",
    "                 I0: torch.Tensor,\n",
    "                 I1: torch.Tensor,\n",
    "                 grid_r: torch.Tensor,\n",
    "                 grid_i: torch.Tensor,\n",
    "                 branch_pts: torch.Tensor,\n",
    "                 mu: torch.Tensor,\n",
    "                 sigma: torch.Tensor,\n",
    "                 branch_cuts=None):\n",
    "        super().__init__()\n",
    "        self.g = I0.shape[0]\n",
    "\n",
    "        # Two sheets\n",
    "        self.register_buffer(\"I0\", _pack_complex_table(I0))   # (1,2g,H,W)\n",
    "        self.register_buffer(\"I1\", _pack_complex_table(I1))   # (1,2g,H,W)\n",
    "\n",
    "        # stats\n",
    "        self.register_buffer(\"mu\",    mu.view(1,1,-1))        # (1,1,2g)\n",
    "        self.register_buffer(\"sigma\", sigma.view(1,1,-1))     # (1,1,2g)\n",
    "        self.gamma = nn.Parameter(torch.tensor(1.0))          # learnable gain\n",
    "\n",
    "        # bounds\n",
    "        self.register_buffer(\"r_min\", torch.tensor(float(grid_r.min())))\n",
    "        self.register_buffer(\"r_max\", torch.tensor(float(grid_r.max())))\n",
    "        self.register_buffer(\"i_min\", torch.tensor(float(grid_i.min())))\n",
    "        self.register_buffer(\"i_max\", torch.tensor(float(grid_i.max())))\n",
    "\n",
    "        # branch points (repulsion)\n",
    "        self.register_buffer(\"bp_real\", branch_pts.real.float())\n",
    "        self.register_buffer(\"bp_imag\", branch_pts.imag.float())\n",
    "\n",
    "        # optional cut penalty\n",
    "        self.has_cuts = (branch_cuts is not None)\n",
    "        if self.has_cuts:\n",
    "            a = np.array([complex(ab[0]) for ab in branch_cuts], dtype=np.complex128)\n",
    "            b = np.array([complex(ab[1]) for ab in branch_cuts], dtype=np.complex128)\n",
    "            self.register_buffer(\"cut_ax\", torch.tensor(a.real, dtype=torch.float32))\n",
    "            self.register_buffer(\"cut_ay\", torch.tensor(a.imag, dtype=torch.float32))\n",
    "            self.register_buffer(\"cut_bx\", torch.tensor(b.real, dtype=torch.float32))\n",
    "            self.register_buffer(\"cut_by\", torch.tensor(b.imag, dtype=torch.float32))\n",
    "\n",
    "    def _map_raw_to_bounds(self, raw_xy):\n",
    "        xr = self.r_min + (self.r_max - self.r_min) * torch.sigmoid(raw_xy[..., 0])\n",
    "        yi = self.i_min + (self.i_max - self.i_min) * torch.sigmoid(raw_xy[..., 1])\n",
    "        return xr, yi\n",
    "\n",
    "    def _norm_to_grid(self, xr, yi):\n",
    "        gx = 2.0 * (xr - self.r_min) / (self.r_max - self.r_min) - 1.0\n",
    "        gy = 2.0 * (yi - self.i_min) / (self.i_max - self.i_min) - 1.0\n",
    "        return gx, gy\n",
    "\n",
    "    def forward(self, raw_xy: torch.Tensor, sheet_logits: torch.Tensor, return_aux=True):\n",
    "        B, g, _ = raw_xy.shape\n",
    "        assert g == self.g\n",
    "\n",
    "        xr, yi = self._map_raw_to_bounds(raw_xy)\n",
    "        gx, gy = self._norm_to_grid(xr, yi)\n",
    "        grid = torch.stack([gx, gy], dim=-1).view(B*g, 1, 1, 2)\n",
    "\n",
    "        # Sample both sheets\n",
    "        I0_s = F.grid_sample(self.I0.expand(B*g, -1, -1, -1),\n",
    "                             grid, mode=\"bilinear\", align_corners=True).view(B, g, -1)\n",
    "        I1_s = F.grid_sample(self.I1.expand(B*g, -1, -1, -1),\n",
    "                             grid, mode=\"bilinear\", align_corners=True).view(B, g, -1)\n",
    "\n",
    "        # Sheet selector: sign=tanh(logit), w0=(1+sign)/2\n",
    "        sign = torch.tanh(sheet_logits).unsqueeze(-1)   # (B,g,1)\n",
    "        w0   = 0.5 * (1.0 + sign)                       # (B,g,1)\n",
    "        I    = w0 * I0_s + (1.0 - w0) * I1_s            # (B,g,2g)\n",
    "\n",
    "        # Standardize and sum over points\n",
    "        I_std  = (I - self.mu) / self.sigma\n",
    "        coords = self.gamma * I_std.sum(dim=1)          # (B,2g)\n",
    "\n",
    "        aux = None\n",
    "        if return_aux:\n",
    "            # boundary penalty in normalized coords\n",
    "            margin = 0.95\n",
    "            bpen = ((gx.abs() - margin).clamp_min(0)**2 +\n",
    "                    (gy.abs() - margin).clamp_min(0)**2).mean()\n",
    "\n",
    "            # branch-point repulsion in physical coords\n",
    "            dx = xr.unsqueeze(-1) - self.bp_real\n",
    "            dy = yi.unsqueeze(-1) - self.bp_imag\n",
    "            d2 = dx*dx + dy*dy\n",
    "            tau = 0.07\n",
    "            rpen = torch.exp(-d2 / (2*tau*tau)).mean()\n",
    "\n",
    "            # optional: cut-distance penalty\n",
    "            cpen = torch.zeros((), device=xr.device)\n",
    "            if self.has_cuts:\n",
    "                px = xr.unsqueeze(-1)  # (B,g,1)\n",
    "                py = yi.unsqueeze(-1)\n",
    "                ax = self.cut_ax.view(1,1,-1)\n",
    "                ay = self.cut_ay.view(1,1,-1)\n",
    "                bx = self.cut_bx.view(1,1,-1)\n",
    "                by = self.cut_by.view(1,1,-1)\n",
    "\n",
    "                vx = bx - ax\n",
    "                vy = by - ay\n",
    "                wx = px - ax\n",
    "                wy = py - ay\n",
    "                vv = (vx*vx + vy*vy).clamp_min(1e-12)\n",
    "                t = (wx*vx + wy*vy) / vv\n",
    "                t = t.clamp(0.0, 1.0)\n",
    "                cx = ax + t*vx\n",
    "                cy = ay + t*vy\n",
    "                d2seg = (px - cx)**2 + (py - cy)**2\n",
    "                d2min = d2seg.min(dim=-1).values\n",
    "                tau_cut = 0.07\n",
    "                cpen = torch.exp(-d2min / (2*tau_cut*tau_cut)).mean()\n",
    "\n",
    "            aux = {\n",
    "                \"x\": xr, \"y\": yi, \"gx\": gx, \"gy\": gy,\n",
    "                \"bound_penalty\": bpen,\n",
    "                \"branch_penalty\": rpen,\n",
    "                \"cut_penalty\": cpen,\n",
    "                \"sheet_sign\": sign.squeeze(-1),\n",
    "                \"sheet_w0\": w0.squeeze(-1),\n",
    "            }\n",
    "\n",
    "        return coords, aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef22d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title AJMNIST_Anchored + axis-periodic head (structure preserved)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AJMNIST_Anchored(nn.Module):\n",
    "    def __init__(self, genus, I0, I1, grid_r, grid_i, branch_pts,\n",
    "                 anchors_xy, mu, sigma, embed_dim=8, branch_cuts=None):\n",
    "        super().__init__()\n",
    "        self.genus = genus\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.AdaptiveAvgPool2d((1,1))\n",
    "        )\n",
    "\n",
    "        self.embed = nn.Parameter(torch.empty(genus, embed_dim))\n",
    "        nn.init.uniform_(self.embed, -2.0, 2.0)\n",
    "\n",
    "        # shared head (no bias) + per-point bias\n",
    "        self.point_head = nn.Linear(64 + embed_dim, 3, bias=False)\n",
    "        nn.init.xavier_uniform_(self.point_head.weight)\n",
    "        self.point_bias = nn.Parameter(torch.zeros(genus, 3))\n",
    "\n",
    "        # Faithful AJ activation\n",
    "        self.aj = AJGridActivationNorm(I0, I1, grid_r, grid_i, branch_pts, mu, sigma, branch_cuts=branch_cuts)\n",
    "        self.classifier = nn.Linear(2*genus, 10)\n",
    "\n",
    "        # ω-aware initialization for biases\n",
    "        rmin, rmax = float(grid_r.min()), float(grid_r.max())\n",
    "        imin, imax = float(grid_i.min()), float(grid_i.max())\n",
    "\n",
    "        def logit(p): return float(torch.log(p/(1-p)))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in range(genus):\n",
    "                x0, y0 = float(anchors_xy[i,0]), float(anchors_xy[i,1])\n",
    "                px = (x0 - rmin) / (rmax - rmin)\n",
    "                py = (y0 - imin) / (imax - imin)\n",
    "                self.point_bias[i, 0] = logit(torch.tensor(px))\n",
    "                self.point_bias[i, 1] = logit(torch.tensor(py))\n",
    "\n",
    "                # Near-binary sheet init: sign=tanh(logit) ≈ ±0.8 ⇒ w0≈0.9 or 0.1\n",
    "                target_sign = 0.8 if (i % 2 == 0) else -0.8\n",
    "                self.point_bias[i, 2] = float(torch.atanh(torch.tensor(target_sign)))\n",
    "\n",
    "    def forward(self, x, return_aux=False):\n",
    "        B = x.size(0)\n",
    "        h = self.conv(x).view(B, -1)\n",
    "        h_exp = h.unsqueeze(1).expand(-1, self.genus, -1)\n",
    "        emb   = self.embed.unsqueeze(0).expand(B, -1, -1)\n",
    "\n",
    "        out   = self.point_head(torch.cat([h_exp, emb], dim=2)) + self.point_bias.unsqueeze(0)\n",
    "        raw_xy, sheet_logits = out[..., :2], out[..., 2]\n",
    "        coords, aux = self.aj(raw_xy, sheet_logits, return_aux=True)\n",
    "        logits = self.classifier(coords)\n",
    "        if return_aux:\n",
    "            return logits, aux\n",
    "        return logits\n",
    "\n",
    "# Axis-aligned Fourier torus features (mock periods)\n",
    "class TorusFeatures(nn.Module):\n",
    "    def __init__(self, dim: int, K: int = 2, freqs=None, learnable: bool = False):\n",
    "        super().__init__()\n",
    "        if freqs is None:\n",
    "            freqs = torch.tensor([0.5, 1.0], dtype=torch.float32)[:K]\n",
    "        else:\n",
    "            freqs = torch.as_tensor(freqs, dtype=torch.float32)[:K]\n",
    "        if learnable:\n",
    "            self.freqs = nn.Parameter(freqs)\n",
    "        else:\n",
    "            self.register_buffer(\"freqs\", freqs)\n",
    "        self.dim, self.K = dim, len(freqs)\n",
    "\n",
    "    def forward(self, u):\n",
    "        B, D = u.shape\n",
    "        f = self.freqs.view(1,1,-1).to(u.device)\n",
    "        ang = u.unsqueeze(-1) * f\n",
    "        return torch.cat([torch.cos(ang), torch.sin(ang)], dim=-1).view(B, D*2*self.K)\n",
    "\n",
    "class AJMNIST_AxisPeriodic(nn.Module):\n",
    "    def __init__(self, genus, I0, I1, grid_r, grid_i, branch_pts,\n",
    "                 anchors_xy, mu, sigma, embed_dim=8, K=2, learnable_freqs=False, branch_cuts=None):\n",
    "        super().__init__()\n",
    "        self.base = AJMNIST_Anchored(genus, I0, I1, grid_r, grid_i, branch_pts,\n",
    "                                     anchors_xy, mu, sigma, embed_dim=embed_dim, branch_cuts=branch_cuts)\n",
    "        D = 2*genus\n",
    "        self.torus = TorusFeatures(D, K=K, learnable=learnable_freqs)\n",
    "        self.classifier = nn.Linear(D*2*K, 10)\n",
    "\n",
    "    @property\n",
    "    def genus(self): return self.base.genus\n",
    "\n",
    "    def forward(self, x, return_aux=False):\n",
    "        B = x.size(0)\n",
    "        h = self.base.conv(x).view(B, -1)\n",
    "        h_exp = h.unsqueeze(1).expand(-1, self.genus, -1)\n",
    "        emb   = self.base.embed.unsqueeze(0).expand(B, -1, -1)\n",
    "        out   = self.base.point_head(torch.cat([h_exp, emb], dim=2)) + self.base.point_bias.unsqueeze(0)\n",
    "        raw_xy, sheet_logits = out[..., :2], out[..., 2]\n",
    "\n",
    "        coords, aux = self.base.aj(raw_xy, sheet_logits, return_aux=True)\n",
    "        feats = self.torus(coords)\n",
    "        logits = self.classifier(feats)\n",
    "        if return_aux:\n",
    "            return logits, aux\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd589dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title MNIST data loaders\n",
    "tfm = T.Compose([T.ToTensor(), T.Normalize((0.1307,), (0.3081,))])\n",
    "train_ds = torchvision.datasets.MNIST(root=\"/content/data\", train=True, download=True, transform=tfm)\n",
    "test_ds  = torchvision.datasets.MNIST(root=\"/content/data\", train=False, download=True, transform=tfm)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2, pin_memory=True)\n",
    "test_loader  = torch.utils.data.DataLoader(test_ds,  batch_size=256, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(\"Train batches:\", len(train_loader), \" Test batches:\", len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f0559f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title AMP-aware training helpers\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "USE_AMP = (device.type == \"cuda\")\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n",
    "ce = nn.CrossEntropyLoss()\n",
    "\n",
    "def count_params(m: nn.Module) -> int:\n",
    "    return sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch(model: nn.Module, loader):\n",
    "    model.eval()\n",
    "    tot, correct, n = 0.0, 0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        loss = ce(logits, y)\n",
    "        tot += loss.item() * x.size(0)\n",
    "        correct += (logits.argmax(1) == y).sum().item()\n",
    "        n += x.size(0)\n",
    "    return tot/n, 100.0*correct/n\n",
    "\n",
    "def train_epoch_amp(model: nn.Module, loader, opt,\n",
    "                    clip: float = 1.0,\n",
    "                    lam_branch: float = 1e-3,\n",
    "                    lam_bound:  float = 1e-3,\n",
    "                    lam_cut:    float = 0.0):\n",
    "    model.train()\n",
    "    tot, correct, n = 0.0, 0, 0\n",
    "\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=USE_AMP):\n",
    "            logits, aux = model(x, return_aux=True)\n",
    "            loss = ce(logits, y)\n",
    "            loss = loss + lam_branch * aux.get(\"branch_penalty\", 0.0)                          + lam_bound  * aux.get(\"bound_penalty\",  0.0)                          + lam_cut    * aux.get(\"cut_penalty\",    0.0)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        if clip is not None:\n",
    "            scaler.unscale_(opt)\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        scaler.step(opt)\n",
    "        scaler.update()\n",
    "\n",
    "        tot += loss.item() * x.size(0)\n",
    "        correct += (logits.argmax(1) == y).sum().item()\n",
    "        n += x.size(0)\n",
    "\n",
    "    return tot/n, 100.0*correct/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab77326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Train faithful axis-periodic AJ model (GPU + AMP)\n",
    "# Move tables/stats to device\n",
    "I0_dev = I0.to(device)\n",
    "I1_dev = I1.to(device)\n",
    "grid_r_dev = grid_r.to(device)\n",
    "grid_i_dev = grid_i.to(device)\n",
    "branch_pts_dev = branch_pts_t.to(device)\n",
    "anchors_xy_dev = anchors_xy_t.to(device)\n",
    "mu_dev = mu_t.to(device)\n",
    "sigma_dev = sigma_t.to(device)\n",
    "\n",
    "# Smaller-batch loaders (OOM-safe)\n",
    "BASE_BS = 64\n",
    "train_loader_small = DataLoader(train_ds, batch_size=BASE_BS, shuffle=True,\n",
    "                                num_workers=2, pin_memory=True, drop_last=True)\n",
    "test_loader_small  = DataLoader(test_ds,  batch_size=256, shuffle=False,\n",
    "                                num_workers=2, pin_memory=True)\n",
    "\n",
    "# Instantiate model\n",
    "K = 2\n",
    "EMBED_DIM = 4\n",
    "LEARN_FREQS = False\n",
    "\n",
    "aj_axis = AJMNIST_AxisPeriodic(\n",
    "    genus,\n",
    "    I0_dev, I1_dev,\n",
    "    grid_r_dev, grid_i_dev,\n",
    "    branch_pts_dev,\n",
    "    anchors_xy_dev,\n",
    "    mu_dev, sigma_dev,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    K=K,\n",
    "    learnable_freqs=LEARN_FREQS,\n",
    "    branch_cuts=branch_cuts\n",
    ").to(device)\n",
    "\n",
    "print(f\"AJ axis-periodic faithful (g={genus}, K={K}) params: {count_params(aj_axis):,}\")\n",
    "\n",
    "# Two-tier LR (same spirit as your existing code)\n",
    "fast_params = list(aj_axis.base.point_head.parameters()) +               [aj_axis.base.point_bias] +               list(aj_axis.torus.parameters()) +               list(aj_axis.classifier.parameters())\n",
    "\n",
    "base_params = list(aj_axis.base.conv.parameters()) +               list(aj_axis.base.aj.parameters())\n",
    "\n",
    "opt = torch.optim.AdamW(\n",
    "    [\n",
    "        {\"params\": base_params, \"lr\": 3e-4},\n",
    "        {\"params\": fast_params, \"lr\": 1e-3},\n",
    "    ],\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "# Train\n",
    "EPOCHS     = 8\n",
    "CLIP_NORM  = 1.0\n",
    "LAM_BRANCH = 1e-3\n",
    "LAM_BOUND  = 1e-3\n",
    "LAM_CUT    = 0.0   # keep 0 by default; set ~1e-4..1e-3 if you want seam avoidance\n",
    "\n",
    "print(\"Starting training | AMP =\", USE_AMP)\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    tr_loss, tr_acc = train_epoch_amp(\n",
    "        aj_axis, train_loader_small, opt,\n",
    "        clip=CLIP_NORM,\n",
    "        lam_branch=LAM_BRANCH,\n",
    "        lam_bound=LAM_BOUND,\n",
    "        lam_cut=LAM_CUT\n",
    "    )\n",
    "    te_loss, te_acc = eval_epoch(aj_axis, test_loader_small)\n",
    "    print(f\"[Faithful AJ axis K={K}] Epoch {ep:02d} | \"\n",
    "          f\"train {tr_loss:.4f} / {tr_acc:.2f}% | \"\n",
    "          f\"test {te_loss:.4f} / {te_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52818d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Quick diagnostics: sheet usage + penalties\n",
    "@torch.no_grad()\n",
    "def batch_diag(model, loader, n_batches=3):\n",
    "    model.eval()\n",
    "    sheets = []\n",
    "    bpen = []\n",
    "    rpen = []\n",
    "    cpen = []\n",
    "    for i, (x,y) in enumerate(loader):\n",
    "        if i >= n_batches:\n",
    "            break\n",
    "        x = x.to(device)\n",
    "        logits, aux = model(x, return_aux=True)\n",
    "        sheets.append(aux[\"sheet_w0\"].mean().item())\n",
    "        bpen.append(aux[\"bound_penalty\"].item())\n",
    "        rpen.append(aux[\"branch_penalty\"].item())\n",
    "        cpen.append(aux.get(\"cut_penalty\", torch.tensor(0.0)).item())\n",
    "    print(f\"Mean sheet w0 over {n_batches} batches: {np.mean(sheets):.3f} (0→sheet1, 1→sheet0)\")\n",
    "    print(f\"Bound penalty:  {np.mean(bpen):.6f}\")\n",
    "    print(f\"Branch penalty: {np.mean(rpen):.6f}\")\n",
    "    print(f\"Cut penalty:    {np.mean(cpen):.6f}\")\n",
    "\n",
    "batch_diag(aj_axis, train_loader_small, n_batches=3)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
