We are developing a novel architecture based on the Abel-Jacobi (AJ) map, which maps rich topology (genus g Riemann surface) to high dimensionality (its Jacobian torus). The idea is to save parameters by having the network expand from a 2d layer to a 2*genus dimensional layer using the AJ map. This map is surjective and so should be able to fill out the full 2g dimensional space of activations, as opposed to the fiducial situation where we just project to 2d and then expand with weight matrices, since in the latter (fiducial) case the network is forced into a 2d representation. In this way, the idea is to work with 2 by 2g dimensional weight matrices, rather than 2g by 2g dimensional ones, which would save a lot of parameters. 

As it stands, our pipeline nicely beats the 2d projection model and is quite competitive with the 2g x 2g baseline. It consists of (1) a notebook which generates the right number of cuts for a genus g hyperelliptic Riemann surface, and pre-computes the Abel-Jacobi integrals on a grid, and (2) a training notebook which defines the AJ activation and implements it. The pipeline here is not completely faithful to the AJ map yet, because the integrands do not change sign upon crossing a cut, which should be relatively easy to upgrade next.  

A control model to consider, in addition to the 2g x 2g baseline and the 2d projection model, is a random 2 \to 2g activation.  We have also developed a random fourier features model to compare to --it's doing a bit worse on MNIST but we need to check this comparison in more detail.  
